hash_id,title,authors,published,summary,pdf_url,entry_id,referenceCount,citationCount,s2FieldsOfStudy,tldr,citations,references
1887624095,SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models,"Muyang Li, Yujun Lin, Zhekai Zhang, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, Song Han",2024-11-07 18:59:58+00:00,"Diffusion models have been proven highly effective at generating high-quality
images. However, as these models grow larger, they require significantly more
memory and suffer from higher latency, posing substantial challenges for
deployment. In this work, we aim to accelerate diffusion models by quantizing
their weights and activations to 4 bits. At such an aggressive level, both
weights and activations are highly sensitive, where conventional post-training
quantization methods for large language models like smoothing become
insufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit
quantization paradigm. Different from smoothing which redistributes outliers
between weights and activations, our approach absorbs these outliers using a
low-rank branch. We first consolidate the outliers by shifting them from
activations to weights, then employ a high-precision low-rank branch to take in
the weight outliers with Singular Value Decomposition (SVD). This process eases
the quantization on both sides. However, na\""{\i}vely running the low-rank
branch independently incurs significant overhead due to extra data movement of
activations, negating the quantization speedup. To address this, we co-design
an inference engine Nunchaku that fuses the kernels of the low-rank branch into
those of the low-bit branch to cut off redundant memory access. It can also
seamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for
re-quantization. Extensive experiments on SDXL, PixArt-$\Sigma$, and FLUX.1
validate the effectiveness of SVDQuant in preserving image quality. We reduce
the memory usage for the 12B FLUX.1 models by 3.5$\times$, achieving
3.0$\times$ speedup over the 4-bit weight-only quantized baseline on the 16GB
laptop 4090 GPU, paving the way for more interactive applications on PCs. Our
quantization library and inference engine are open-sourced.",http://arxiv.org/pdf/2411.05007v1,http://arxiv.org/abs/2411.05007v1,87,0,"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","{'model': 'tldr@v2.0.0', 'text': 'This work aims to accelerate diffusion models by quantizing their weights and activations to 4 bits by proposing SVDQuant, a new 4-bit quantization paradigm, and co-designs an inference engine Nunchaku that fuses the kernels of the low-rank branch into those of the low-bit branch to cut off redundant memory access.'}",[],"[{'paperId': '2b2bb74357be0123d36594c8a0a1dda9673d3631', 'title': 'LinFusion: 1 GPU, 1 Minute, 16K Image'}, {'paperId': '6df92ef64a8b18db8ad4e6e282c42ec6698113bd', 'title': 'From GaLore to WeLore: How Low-Rank Weights Non-uniformly Emerge from Low-Rank Gradients'}, {'paperId': 'c609f1f1ec2701e5efc3fe3a6aa66574e3d321a7', 'title': 'AsyncDiff: Parallelizing Diffusion Models by Asynchronous Denoising'}, {'paperId': '5de435d88851c9addb54631e8e63a02def3ee024', 'title': 'BitsFusion: 1.99 bits Weight Quantization of Diffusion Model'}, {'paperId': 'fbb7b1b301c6b45b82638058834a5304480d41b6', 'title': 'ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation'}, {'paperId': '2cad80abadf04b7950de237ef0c4a6d5c757ec1f', 'title': 'Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching'}, {'paperId': '640ef31482ae637435ec0079f03d3139a8be9520', 'title': 'HQ-DiT: Efficient Diffusion Transformer with FP4 Hybrid Quantization'}, {'paperId': 'a2df3d49d5d0592efc1ab2b55cdf66dea1c2a6a5', 'title': 'MixDQ: Memory-Efficient Few-Step Text-to-Image Diffusion Models with Metric-Decoupled Mixed Precision Quantization'}, {'paperId': 'a7ed153f848cd8a299c866f50d74b55f16d606ce', 'title': 'SpinQuant: LLM quantization with learned rotations'}, {'paperId': '0365ad3bd3543c330a2dda5c679355cd74a2a372', 'title': 'PTQ4DiT: Post-training Quantization for Diffusion Transformers'}, {'paperId': 'cc6fc3c546b354abf6a0aa3b553f28a6b812489f', 'title': 'Improved Distribution Matching Distillation for Fast Image Synthesis'}, {'paperId': '0dc9836b159c752c801f55d63f08ddfb9f5140e8', 'title': 'Distilling Diffusion Models into Conditional GANs'}, {'paperId': '0f0d757e764a7f21d7aaa329c835571b247dd937', 'title': 'QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving'}, {'paperId': 'a8172972b31dc3f8b9e848dcf65e21819e77fba0', 'title': 'Condition-Aware Neural Network for Controlled Image Generation'}, {'paperId': 'dd85e6cab147d237a0b1ab6f674570d3efb4d4a0', 'title': 'QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs'}, {'paperId': 'c1fa6255cc9fc3128f74befc7855e255bc7a2c6e', 'title': 'GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection'}, {'paperId': '41a66997ce0a366bba3becf7c3f37c9aebb13fbd', 'title': 'Scaling Rectified Flow Transformers for High-Resolution Image Synthesis'}, {'paperId': '29053ced75c38a121f2a76ffb36880614c7e188a', 'title': 'DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models'}, {'paperId': 'bdb826b267841b8948497064efd01b781ac10341', 'title': 'Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in Text-to-Image Generation'}, {'paperId': '97465306913b8fd3492041b8d38d86faddfc12ff', 'title': 'QuEST: Low-bit Diffusion Model Quantization via Efficient Selective Finetuning'}, {'paperId': 'e22e9d6e86045f99e02d6b379fb728975913b0a2', 'title': 'Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models'}, {'paperId': 'a17fe25540a96782cd1f24d7be512f7516359a7f', 'title': 'A Picture is Worth More Than 77 Text Tokens: Evaluating CLIP-Style Models on Dense Captions'}, {'paperId': 'a1bcf68d6ed2fec1ecaf16b67f2d19bc20c00ee6', 'title': 'ASVD: Activation-aware Singular Value Decomposition for Compressing Large Language Models'}, {'paperId': '4ddfdf3747c70ae9267cb78a7ff7772ef3148803', 'title': 'Efficient Quantization Strategies for Latent Diffusion Models'}, {'paperId': '1f454157062dd48b6065e75cd3fc97d4cd906a04', 'title': 'DeepCache: Accelerating Diffusion Models for Free'}, {'paperId': '516a4c05883d4dc130bb7a638ad8e427b6756aac', 'title': 'One-Step Diffusion with Distribution Matching Distillation'}, {'paperId': 'd730d42bb655b3b44727d71c147f9758612043a8', 'title': 'Adversarial Diffusion Distillation'}, {'paperId': '707f027590ec74836ce7fb5334f0a2a8bd32f7c6', 'title': 'TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models'}, {'paperId': '0d2f828efd1efdd57c3c4455d09413f2d3b5c11f', 'title': 'LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning'}, {'paperId': '0a851958ba955e125bfd8486da481c42764146a0', 'title': 'Post-training Quantization with Progressive Calibration and Activation Relaxing for Text-to-Image Diffusion Models'}, {'paperId': '9529e50807f36acf3d2e4af994b5803c47e4746a', 'title': 'Atom: Low-bit Quantization for Efficient and Accurate LLM Serving'}, {'paperId': '9d78505dd333b4bc77b75b65313ab9f96bcfe198', 'title': 'Microscaling Data Formats for Deep Learning'}, {'paperId': 'af8123ecdff838f63e4eba0b36b8babe4c5cee65', 'title': 'LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models'}, {'paperId': '8b7cce220c3b19f9b2d4a6c531907ed3b592b55e', 'title': 'Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference'}, {'paperId': '1c044b2bd2487feef414b7b9f3570d7e4e938d40', 'title': 'EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models'}, {'paperId': '945db0077b6d19b720f5998b3f61300013c4f885', 'title': 'QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models'}, {'paperId': 'd7890d1906d95c4ae4c430b350455156d6d8aed9', 'title': 'SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis'}, {'paperId': 'bc8428e270a5474cabfaff578d44955f757ccacd', 'title': 'LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation'}, {'paperId': '3b7ef6f9f27e33e6a4e3bfac90dcb01ab09718bc', 'title': 'SqueezeLLM: Dense-and-Sparse Quantization'}, {'paperId': 'db9507cdd3e2d7d9c90ed185bd831e55c62dcec9', 'title': 'AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration'}, {'paperId': '9b504916f0e8fbeb1891f0db299dcbbc118f4898', 'title': 'SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds'}, {'paperId': 'c06572c7bf0fdedbb4e719e44d7fb6bba2b486a9', 'title': 'Towards Accurate Post-Training Quantization for Diffusion Models'}, {'paperId': '32ac52069e562d4f900afee70bdca63f53461481', 'title': 'QLoRA: Efficient Finetuning of Quantized LLMs'}, {'paperId': '936d32864240bde74f5824df3bd8343d8de08843', 'title': 'PTQD: Accurate Post-Training Quantization for Diffusion Models'}, {'paperId': '1b2355c3c674b26a977768a91a164384ad51bbb1', 'title': 'ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation'}, {'paperId': '8f48c75e1354c88a84a67abb60789083c12e5037', 'title': 'ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation'}, {'paperId': 'ac974291d7e3a152067382675524f3e3c2ded11b', 'title': 'Consistency Models'}, {'paperId': 'efbe97d20c4ffe356e8826c01dc550bacc405add', 'title': 'Adding Conditional Control to Text-to-Image Diffusion Models'}, {'paperId': '61e721334296ebfbbf6443b5ed9eb8c83b708c95', 'title': 'Scaling Vision Transformers to 22 Billion Parameters'}, {'paperId': '489ab1945feb21f17b3efbcf40726c8cbb52bb75', 'title': 'Q-Diffusion: Quantizing Diffusion Models'}, {'paperId': '736973165f98105fec3729b7db414ae4d80fcbeb', 'title': 'Scalable Diffusion Models with Transformers'}, {'paperId': 'd5e0ee741e953d857263f70787449e4a57fc1c8d', 'title': 'Post-Training Quantization on Diffusion Models'}, {'paperId': '2c994fadbb84fb960d8306ee138dbeef41a5b323', 'title': 'SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models'}, {'paperId': '34ea83793ae149d24fdb35f7d20c911cb98f72ea', 'title': 'Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models'}, {'paperId': 'e24f4b28167b05fbf7d29000490fc0a4e4c109c7', 'title': 'eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers'}, {'paperId': '625d57bd52c60cd79aa4add6c4420dc2ad3b808a', 'title': 'On Distillation of Guided Diffusion Models'}, {'paperId': '897f3bb5eacaa80359e81ff33378e1110e20ae95', 'title': 'All are Worth Words: A ViT Backbone for Diffusion Models'}, {'paperId': '5b19bf6c3f4b25cac96362c98b930cf4b37f6744', 'title': 'DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation'}, {'paperId': '03ae89e796b1a8b566ae1554fab65c8c88b3a55f', 'title': 'Exploring CLIP for Assessing the Look and Feel of Images'}, {'paperId': 'd151ced8700d84a2efe411a234a4cb2c595e8ca9', 'title': 'Language model compression with weighted low-rank factorization'}, {'paperId': '670bab7b71be5e432b0dc60f406a6115cf6c0633', 'title': 'gDDIM: Generalized denoising diffusion implicit models'}, {'paperId': '4530c25da949bb2185c50663158ef19d52e3c6b5', 'title': 'DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps'}, {'paperId': '6651ff3d8c89a3cddfeef0c5d03ac4cb121758e3', 'title': 'Fast Sampling of Diffusion Models with Exponential Integrator'}, {'paperId': 'c10075b3746a9f3dd5811970e93c8ca3ad39b39d', 'title': 'High-Resolution Image Synthesis with Latent Diffusion Models'}, {'paperId': 'f671a09e3e5922e6d38cb77dda8d76d5ceac2a27', 'title': 'SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations'}, {'paperId': 'a8ca46b171467ceb2d7652fbfb67fe701ad86092', 'title': 'LoRA: Low-Rank Adaptation of Large Language Models'}, {'paperId': '38b0567e83386ddc294d6c81b541deacbd8e3c2a', 'title': 'CLIPScore: A Reference-free Evaluation Metric for Image Captioning'}, {'paperId': '6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4', 'title': 'Learning Transferable Visual Models From Natural Language Supervision'}, {'paperId': '5c126ae3421f05768d8edd97ecd44b1364e2c99a', 'title': 'Denoising Diffusion Probabilistic Models'}, {'paperId': 'b6207686ecf40f1eb1752425299f265dffbc4abe', 'title': 'GAN Compression: Efficient Architectures for Interactive Conditional GANs'}, {'paperId': 'c468bbde6a22d961829e1970e6ad5795e05418d1', 'title': 'The Unreasonable Effectiveness of Deep Features as a Perceptual Metric'}, {'paperId': '231af7dc01a166cac3b5b01ca05778238f796e41', 'title': 'GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium'}, {'paperId': '2c03df8b48bf3fa39054345bafabfeff15bfd11d', 'title': 'Deep Residual Learning for Image Recognition'}, {'paperId': '6364fdaa0a0eccd823a779fcdd489173f938e91a', 'title': 'U-Net: Convolutional Networks for Biomedical Image Segmentation'}, {'paperId': '696ca58d93f6404fea0fc75c62d1d7b378f47628', 'title': 'Microsoft COCO Captions: Data Collection and Evaluation Server'}, {'paperId': '2dcef55a07f8607a819c21fe84131ea269cc2e3c', 'title': 'Deep Unsupervised Learning using Nonequilibrium Thermodynamics'}, {'paperId': '2016c5b269e2011afba84670c5f85dc7f96644ff', 'title': 'PipeFusion: Displaced Patch Pipeline Parallelism for Inference of Diffusion Transformer Models'}, {'paperId': 'b8b45b14df9029562b8995c6ab7fd90a8810f312', 'title': 'GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale'}, {'paperId': None, 'title': 'Concentration inequalities and model selection: Ecole d’Eté de Probabilités de Saint-Flour XXXIII-2003'}, {'paperId': None, 'title': 'knight in shining armor, standing in front of a castle under'}, {'paperId': None, 'title': 'NVIDIA'}, {'paperId': None, 'title': 'A handsome man in a suit, 25 years old, cool, futuristic'}, {'paperId': None, 'title': 'major update]'}, {'paperId': None, 'title': 'GHIBSKY'}, {'paperId': None, 'title': ': Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models'}, {'paperId': None, 'title': 'Black-Forest-Labs'}, {'paperId': None, 'title': 'surrounded by towering pine trees and rocky cliffs (cid:44) →'}]"
1929777137,Diff-2-in-1: Bridging Generation and Dense Perception with Diffusion Models,"Shuhong Zheng, Zhipeng Bao, Ruoyu Zhao, Martial Hebert, Yu-Xiong Wang",2024-11-07 18:59:53+00:00,"Beyond high-fidelity image synthesis, diffusion models have recently
exhibited promising results in dense visual perception tasks. However, most
existing work treats diffusion models as a standalone component for perception
tasks, employing them either solely for off-the-shelf data augmentation or as
mere feature extractors. In contrast to these isolated and thus sub-optimal
efforts, we introduce a unified, versatile, diffusion-based framework,
Diff-2-in-1, that can simultaneously handle both multi-modal data generation
and dense visual perception, through a unique exploitation of the
diffusion-denoising process. Within this framework, we further enhance
discriminative visual perception via multi-modal generation, by utilizing the
denoising network to create multi-modal data that mirror the distribution of
the original training set. Importantly, Diff-2-in-1 optimizes the utilization
of the created diverse and faithful data by leveraging a novel self-improving
learning mechanism. Comprehensive experimental evaluations validate the
effectiveness of our framework, showcasing consistent performance improvements
across various discriminative backbones and high-quality multi-modal data
generation characterized by both realism and usefulness.",http://arxiv.org/pdf/2411.05005v1,http://arxiv.org/abs/2411.05005v1,93,0,"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","{'model': 'tldr@v2.0.0', 'text': 'This work introduces a unified, versatile, diffusion-based framework, Diff-2-in-1, that can simultaneously handle both multi-modal data generation and dense visual perception, through a unique exploitation of the diffusion-denoising process.'}",[],"[{'paperId': '516a4c05883d4dc130bb7a638ad8e427b6756aac', 'title': 'One-Step Diffusion with Distribution Matching Distillation'}, {'paperId': '87b267c58f4c9f14bda3912a85fac091a1d5adab', 'title': 'Multi-task View Synthesis with Neural Radiance Fields'}, {'paperId': '448e7e873a6e520072301b7d9c1d0d9289778418', 'title': 'InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation'}, {'paperId': '734101311a8ae392ded894696ca070b04b82575f', 'title': 'Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning'}, {'paperId': '311a798c65e9824bf82d93799988e26e8c27e67f', 'title': 'Consistent Multimodal Generation via A Unified GAN Framework'}, {'paperId': 'f421b314aaff48e463507034691cfdd3f93cd4c2', 'title': 'Emergent Correspondence from Image Diffusion'}, {'paperId': '842182174ebd9e070101c85aa16c0818e5363c42', 'title': 'The Surprising Effectiveness of Diffusion Models for Optical Flow and Monocular Depth Estimation'}, {'paperId': '3ad0899e9fa71cc6c294db4e8061ff6bc9e22225', 'title': 'A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence'}, {'paperId': 'e49b1b6227afbe16f01174a72dbf2868915f5aac', 'title': 'Unsupervised Semantic Correspondence Using Stable Diffusion'}, {'paperId': 'c267f83a5d0fb0e4ed4c5c1174998ab6efd457aa', 'title': 'Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence'}, {'paperId': 'c22dfd7dd7364d96e03e47998819cc188a53d3f4', 'title': 'Image retrieval outperforms diffusion models on data augmentation'}, {'paperId': '6ab25d7ac1024fa5ed26514294bfbb3b77ab24f9', 'title': 'iDisc: Internal Discretization for Monocular Depth Estimation'}, {'paperId': '2b50e72ffd2db2915dd1c6bddab710195cc64583', 'title': 'DDP: Diffusion Model for Dense Visual Prediction'}, {'paperId': '4702d5a163477c734a54f3ed2d171dca1504eaae', 'title': 'Your Diffusion Model is Secretly a Zero-Shot Classifier'}, {'paperId': '53974e68d7ea681349e9cd13babb5d511dc86460', 'title': 'Text2Tex: Text-driven Texture Synthesis via Diffusion Models'}, {'paperId': 'c09ca9da1fce13b1560f45c38321c7bb971f13fc', 'title': 'Unleashing Text-to-Image Diffusion Models for Visual Perception'}, {'paperId': 'f9516f2b0e1816e87fd70aeedcbfb12cff341eb2', 'title': 'Monocular Depth Estimation using Diffusion Models'}, {'paperId': 'efbe97d20c4ffe356e8826c01dc550bacc405add', 'title': 'Adding Conditional Control to Text-to-Image Diffusion Models'}, {'paperId': '1e05c5427d6a35a3b1bf37fb955f2ea94995a71d', 'title': 'Effective Data Augmentation With Diffusion Models'}, {'paperId': '3f5b31c4f7350dc88002c121aecbdc82f86eb5bb', 'title': 'BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models'}, {'paperId': '9b6d703561b59adc24476234df0bdde607445289', 'title': 'Open-vocabulary Object Segmentation with Diffusion Models'}, {'paperId': 'e3d3c1321554d7d14eec309e61ba70102b0629e1', 'title': 'DeMT: Deformable Mixer Transformer for Multi-Task Learning of Dense Prediction'}, {'paperId': '7f908fb67c6d4089fd00a35595337feaa13d56a8', 'title': 'Fast Sampling of Diffusion Models via Operator Learning'}, {'paperId': 'e79614372e420f1211ab7afd0676af1a98b3d657', 'title': 'Semantic Image Synthesis via Diffusion Models'}, {'paperId': '21d0f56617c38bd9a5e4794d56b70a1c19d50a09', 'title': 'Beyond RGB: Scene-Property Synthesis with Neural Radiance Fields'}, {'paperId': '4530c25da949bb2185c50663158ef19d52e3c6b5', 'title': 'DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps'}, {'paperId': 'd69ac67b1269083cc334074c264b0a631abee197', 'title': 'Multi-Task Learning With Multi-Query Transformer for Dense Prediction'}, {'paperId': 'e85c905867695ef42e79bcb095b314ad503f7fe5', 'title': 'Inverted Pyramid Multi-task Transformer for Dense Scene Understanding'}, {'paperId': '177e957f5cd93229c9794ea652c646d2557b4a69', 'title': 'A ConvNet for the 2020s'}, {'paperId': 'c10075b3746a9f3dd5811970e93c8ca3ad39b39d', 'title': 'High-Resolution Image Synthesis with Latent Diffusion Models'}, {'paperId': '42f2271cebb7f272b0066c1f22d33381f139ee68', 'title': 'Label-Efficient Semantic Segmentation with Diffusion Models'}, {'paperId': '6d1ef4436904de111c8b1975bbf25d3fe2f165f7', 'title': 'DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting'}, {'paperId': 'a7aa150b55d64d339b1c154d6d88455fc3cbc44f', 'title': 'ClipCap: CLIP Prefix for Image Captioning'}, {'paperId': '6351ebb4a3287f5f3e1273464b3b91e5df5a16d7', 'title': 'Masked Autoencoders Are Scalable Vision Learners'}, {'paperId': '9e5e866c61abe4ad7894ee86eaa6b35ec8a92594', 'title': 'Estimating and Exploiting the Aleatoric Uncertainty in Surface Normal Estimation'}, {'paperId': 'f671a09e3e5922e6d38cb77dda8d76d5ceac2a27', 'title': 'SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations'}, {'paperId': '1157fa88b4a53ff83e450c2dd211adeb03216885', 'title': 'Generative Modeling for Multi-task Visual Learning'}, {'paperId': 'e3d7778a47c6cab4ea1ef3ee9d19ec1510c15c60', 'title': 'SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers'}, {'paperId': '2bf01a8dae190a377afaad90021e6c94ad288058', 'title': 'Exploring Relational Context for Multi-Task Dense Prediction'}, {'paperId': '8e33914d6051dd031a5e096962b9398fc1d16067', 'title': 'Vision Transformers for Dense Prediction'}, {'paperId': 'fd9d3f4de3e340137011ae470aab7591a62b6e38', 'title': 'Unbiased Teacher for Semi-Supervised Object Detection'}, {'paperId': '268d347e8a55b5eb82fb5e7d2f800e33c75ab18a', 'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'}, {'paperId': '014576b866078524286802b1d0e18628520aa886', 'title': 'Denoising Diffusion Implicit Models'}, {'paperId': 'e13a762cf10bf251b80dbca1a97dd57cc59da5ee', 'title': 'GeoNet++: Iterative Geometric Neural Network with Edge-Aware Refinement for Joint Depth and Surface Normal Estimation'}, {'paperId': '5c126ae3421f05768d8edd97ecd44b1364e2c99a', 'title': 'Denoising Diffusion Probabilistic Models'}, {'paperId': '8cdb9e2aaf84d8c60849f1d0a67f000d15e1a19b', 'title': 'Pattern-Structure Diffusion for Multi-Task Learning'}, {'paperId': '54ccb7fe9a5befc845411f474c344ab6c8e731c2', 'title': 'MTI-Net: Multi-Scale Task Interaction Networks for Multi-Task Learning'}, {'paperId': '7bd83b055702bc178aa26def5b6df463f8eab7b9', 'title': 'Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer'}, {'paperId': 'f0e2b45ce640e47a538611fc30274f24d66bfb3c', 'title': 'Pattern-Affinitive Propagation Across Depth, Surface Normal and Semantic Segmentation'}, {'paperId': '8433be13db27f7c195cf54999dfd2dd2845b831e', 'title': 'Attentive Single-Tasking of Multiple Tasks'}, {'paperId': '9e8db1519245426f3a78752a3d8360484f4626b1', 'title': 'OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields'}, {'paperId': 'f8ca96d34427b2595526fa6e852d7864a5205f2b', 'title': 'Generative Adversarial Networks for Unsupervised Monocular Depth Prediction'}, {'paperId': '4f19d33e808a6675f11fb624499d303368deafa1', 'title': 'Learning Monocular Depth by Distilling Cross-domain Stereo Networks'}, {'paperId': 'b9ca3db76702111955ff583527fa40e0a157b796', 'title': 'Unsupervised Adversarial Depth Estimation Using Cycled Generative Networks'}, {'paperId': 'aaab0bd4d79d4f19109bab0fbcdb05070fb0edd1', 'title': 'Unified Perceptual Parsing for Scene Understanding'}, {'paperId': '9c3a8c499a2ad67372d1ffc93bbda8bf4c0c773e', 'title': 'Verisimilar Image Synthesis for Accurate Detection and Recognition of Texts in Scenes'}, {'paperId': '589cfcb2f995c94b0a98c902cc1f5e0f27cbd927', 'title': 'Digging Into Self-Supervised Monocular Depth Estimation'}, {'paperId': '544c477e668dcf9a341525168000b039e83219de', 'title': 'Emotion Classification with Data Augmentation Using Generative Adversarial Networks'}, {'paperId': '8a9c4f1b58258afa2016b0eca0b3bfd2dc2ba3d8', 'title': 'GeoNet: Geometric Neural Network for Joint Depth and Surface Normal Estimation'}, {'paperId': '6cfa4ab327d42103195cb8e5c6181028cec8ee62', 'title': 'PAD-Net: Multi-tasks Guided Prediction-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing'}, {'paperId': '8b6afef69b14b97a272c667b6b9004e441085c89', 'title': 'StarMap for Category-Agnostic Keypoint and Viewpoint Estimation'}, {'paperId': '8c8d0031b24937d8a8ec7a4c5ab5fda4f4797803', 'title': 'NDDR-CNN: Layerwise Feature Fusing in Multi-Task CNNs by Neural Discriminative Dimensionality Reduction'}, {'paperId': '2a5667702b0f1ff77dde8fb3e2e10d4e05e8de9d', 'title': 'Scene Parsing through ADE20K Dataset'}, {'paperId': '7493389667058116dbc7e808987f129325ee60d7', 'title': 'Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results'}, {'paperId': 'e52e37cd91366f07df1f98e88f87010f494dd16e', 'title': 'ScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes'}, {'paperId': 'dfad8f616bd2a05c8cae5f61060f743f966ece85', 'title': 'Realtime Multi-person 2D Pose Estimation Using Part Affinity Fields'}, {'paperId': '2976605dc3b73377696537291d45f09f1ab1fbf5', 'title': 'Cross-Stitch Networks for Multi-task Learning'}, {'paperId': '2c03df8b48bf3fa39054345bafabfeff15bfd11d', 'title': 'Deep Residual Learning for Image Recognition'}, {'paperId': '6364fdaa0a0eccd823a779fcdd489173f938e91a', 'title': 'U-Net: Convolutional Networks for Biomedical Image Segmentation'}, {'paperId': 'cf986bfe13a24d4739f95df3a856a3c6e4ed4c1c', 'title': 'Learning Deconvolution Network for Semantic Segmentation'}, {'paperId': '3f25b3ddef8626ace7aa0865a1a9e3dad1f23fb6', 'title': 'Deep convolutional neural fields for depth estimation from a single image'}, {'paperId': '60a15269fb2e6d031d2dad4bb9c4225b18bbf1eb', 'title': 'Designing deep networks for surface normal estimation'}, {'paperId': 'cb3a2ddcf305e2ec0f6b94af13d1e631ed261bdc', 'title': 'Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-scale Convolutional Architecture'}, {'paperId': '6fc6803df5f9ae505cae5b2f178ade4062c768d0', 'title': 'Fully convolutional networks for semantic segmentation'}, {'paperId': 'e15cf50aa89fee8535703b9f9512fca5bfc43327', 'title': 'Going deeper with convolutions'}, {'paperId': 'c221de2dfebb1a536382c8ad8ced3910b872e6d3', 'title': 'Discriminatively Trained Dense Surface Normal Estimation'}, {'paperId': 'eb42cf88027de515750f230b23b1a057dc782108', 'title': 'Very Deep Convolutional Networks for Large-Scale Image Recognition'}, {'paperId': '3419ccd5c94d301ee08d716d037f0c3c6a62e78e', 'title': 'The Role of Context for Object Detection and Semantic Segmentation in the Wild'}, {'paperId': 'dd2cf76ae78a3262a094ac865aa9f60c55472c5d', 'title': 'Depth Map Prediction from a Single Image using a Multi-Scale Deep Network'}, {'paperId': '2a002ce457f7ab3088fbd2691734f1ce79f750c4', 'title': 'DeepPose: Human Pose Estimation via Deep Neural Networks'}, {'paperId': '1a2a770d23b4a171fa81de62a78a3deb0588f238', 'title': 'Visualizing and Understanding Convolutional Networks'}, {'paperId': 'abd1c342495432171beb7ca8fd9551ef13cbd0ff', 'title': 'ImageNet classification with deep convolutional neural networks'}, {'paperId': 'c1994ba5946456fc70948c549daf62363f13fa2d', 'title': 'Indoor Segmentation and Support Inference from RGBD Images'}, {'paperId': 'd2c733e34d48784a37d717fe43d9e93277a8c53e', 'title': 'ImageNet: A large-scale hierarchical image database'}, {'paperId': '93cf48a7d86a5cb9576481bc0bf4fcc56b6212ef', 'title': 'Generative versus discriminative methods for object recognition'}, {'paperId': '0e0801da1a187d90862cd00ce7f12222ff965ef0', 'title': 'Classification with Hybrid Generative/Discriminative Models'}, {'paperId': '90929a6aa901ba958eb4960aeeb594c752e08369', 'title': 'On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes'}, {'paperId': '54b6f287c1dc7018d6acef5a8df3bcf719112426', 'title': 'Discriminative vs Informative Learning'}, {'paperId': '2953eba822d9d65f754f18076977e42781e988b9', 'title': 'TaskPrompter: Spatial-Channel Multi-Task Prompting for Dense Scene Understanding'}, {'paperId': 'c8b25fab5608c3e033d34b4483ec47e68ba109b7', 'title': 'Swin Transformer: Hierarchical Vision Transformer using Shifted Windows'}, {'paperId': None, 'title': 'Marr Revisited: 2D-3D model alignment via surface normal prediction'}, {'paperId': None, 'title': 'DiffuMask: Syn-thesizing images with pixel-level annotations for semantic segmentation using diffusion models'}, {'paperId': None, 'title': ': Low-rank adaptation of large language models'}]"
2005627715,ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning,"David Junhao Zhang, Roni Paiss, Shiran Zada, Nikhil Karnad, David E. Jacobs, Yael Pritch, Inbar Mosseri, Mike Zheng Shou, Neal Wadhwa, Nataniel Ruiz",2024-11-07 18:59:45+00:00,"Recently, breakthroughs in video modeling have allowed for controllable
camera trajectories in generated videos. However, these methods cannot be
directly applied to user-provided videos that are not generated by a video
model. In this paper, we present ReCapture, a method for generating new videos
with novel camera trajectories from a single user-provided video. Our method
allows us to re-generate the reference video, with all its existing scene
motion, from vastly different angles and with cinematic camera motion. Notably,
using our method we can also plausibly hallucinate parts of the scene that were
not observable in the reference video. Our method works by (1) generating a
noisy anchor video with a new camera trajectory using multiview diffusion
models or depth-based point cloud rendering and then (2) regenerating the
anchor video into a clean and temporally consistent reangled video using our
proposed masked video fine-tuning technique.",http://arxiv.org/pdf/2411.05003v1,http://arxiv.org/abs/2411.05003v1,101,0,"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","{'model': 'tldr@v2.0.0', 'text': 'This paper presents ReCapture, a method for generating new videos with novel camera trajectories from a single user-provided video by generating a noisy anchor video with a new camera trajectory using multiview diffusion models or depth-based point cloud rendering and regenerating the anchor video into a clean and temporally consistent reangled video using the proposed masked video fine-tuning technique.'}",[],"[{'paperId': 'bc2ddba47a9d6b378abd8a8584849f3e4c410f24', 'title': 'WorldSimBench: Towards Video Generation Models as World Simulators'}, {'paperId': 'b2d3c886b49203c1215588c6fedab2316a59457c', 'title': 'Movie Gen: A Cast of Media Foundation Models'}, {'paperId': '13109178062422938161de612c75ee2b333941c2', 'title': 'Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations'}, {'paperId': 'bde2a93e64558fae1b447e6beec0d3624dfc790a', 'title': 'ViewCrafter: Taming Video Diffusion Models for High-fidelity Novel View Synthesis'}, {'paperId': '7b248d78573ccf0dca6aa2cec2743d3eccaa9d1a', 'title': 'CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer'}, {'paperId': 'b60cd86fef2ab6cfda33c9a5c7a7c75b7e8b035b', 'title': 'Shape of Motion: 4D Reconstruction from a Single Video'}, {'paperId': 'af4b0ce1980615c7d72d0a34972a2af2bc6bc0c0', 'title': 'VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control'}, {'paperId': '8482195859611752d4c6f64832683bfa4564d879', 'title': 'Still-Moving: Customized Video Generation without Customized Video Data'}, {'paperId': '6d8d5490aef1db27a60ad33a34d78a117fb32610', 'title': 'Controlling Space and Time with Diffusion Models'}, {'paperId': 'f89aaa2d04c5ed61dcd9dc5dfc35176be31610b1', 'title': 'Magic Insert: Style-Aware Drag-and-Drop'}, {'paperId': '77b9e4693c72bfe9d15d82bd29ca9d23e6fef7eb', 'title': 'Dynamic Gaussian Marbles for Novel View Synthesis of Casual Monocular Videos'}, {'paperId': '7f9cb9a28285b2a29ddc57c99e2da1ad121b1c4e', 'title': 'Training-free Camera Control for Video Generation'}, {'paperId': '3c13686750c9b2541bff9567ea440c96df810764', 'title': 'L4GM: Large 4D Gaussian Reconstruction Model'}, {'paperId': 'd097e5d8d277ab62671abf9ef1cb08f4966a0c46', 'title': '4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models'}, {'paperId': '46412b475b3dddde999b82db2919a4b6f3664583', 'title': 'CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation'}, {'paperId': '1b412ae5392172c5e6e017481b29f6afd9d3ef8a', 'title': '4Diffusion: Multi-view Video Diffusion Model for 4D Generation'}, {'paperId': '8a30d3ccd21d6a59f0b832d8511ef5ba948bb1f9', 'title': 'Collaborative Video Diffusion: Consistent Multi-video Generation with Camera Control'}, {'paperId': '0b7d3bf45a62d0e636e6181ed543c26182b8982b', 'title': 'Generative Camera Dolly: Extreme Monocular Dynamic Novel View Synthesis'}, {'paperId': '4987a76781f299be64ac43419c8b489ca1f4515b', 'title': 'CAT3D: Create Anything in 3D with Multi-View Diffusion Models'}, {'paperId': '6b5fc164c4f21e4a4f151df60bfd5e32b061a903', 'title': 'InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation'}, {'paperId': '71ebcc0d022bc228d04ead6dd3e76d1d88ae4c1a', 'title': 'CameraCtrl: Enabling Camera Control for Text-to-Video Generation'}, {'paperId': '97cb2eb0d0517e34bf4202f0593600bb6fa043cd', 'title': 'Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis'}, {'paperId': '94f7d8bce3bb848d127c8f113afc5bb0243579df', 'title': 'Lumiere: A Space-Time Diffusion Model for Video Generation'}, {'paperId': '492bc8339d8aac442c4ec13f8c1d59e980a3af2f', 'title': 'VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models'}, {'paperId': 'dad2852ee397ae3edd28ad9d3c28e759a3bd93d2', 'title': 'Diffusion Priors for Dynamic View Synthesis from Monocular Videos'}, {'paperId': 'e0eac8c64be3313e581c28a495bec192e7e67284', 'title': 'Latte: Latent Diffusion Transformer for Video Generation'}, {'paperId': '65022cdf3cba83ca2000ce18e694ff54ce2ca787', 'title': 'DreamGaussian4D: Generative 4D Gaussian Splatting'}, {'paperId': '0c4f46e4dcae5527018e6432fb60cfe8c3354e97', 'title': 'VideoPoet: A Large Language Model for Zero-Shot Video Generation'}, {'paperId': '50c51b595dfee1df1a0506ee7acef05ba01d3aa7', 'title': 'Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models'}, {'paperId': '905ba940236b00bebb2fd348d4d932e7887b0c0a', 'title': 'Photorealistic Video Generation with Diffusion Models'}, {'paperId': '44999d9bae3976d38e91b742c4541a21c7000260', 'title': 'MotionCtrl: A Unified and Flexible Motion Controller for Video Generation'}, {'paperId': '114e83828084ed6e82b3984979b2ec8f6e7f9cf7', 'title': 'ReconFusion: 3D Reconstruction with Diffusion Priors'}, {'paperId': '763e798815a7c2f571f9efa7636a94123d4da266', 'title': 'Fast View Synthesis of Casual Videos with Soup-of-Planes'}, {'paperId': 'c320eb9255940cec341af7aef9c8c38a7a167d6a', 'title': 'Style Aligned Image Generation via Shared Attention'}, {'paperId': '4e9a8141da2a8c603722b07d096109207f8e0b66', 'title': 'VBench: Comprehensive Benchmark Suite for Video Generative Models'}, {'paperId': 'ff0baeb316df06d017f381c80a2c3aa6399fe938', 'title': '4D-fy: Text-to-4D Generation Using Hybrid Score Distillation Sampling'}, {'paperId': '1206b05eae5a06ba662ae79fb291b50e359c4f42', 'title': 'Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets'}, {'paperId': 'ecb3c92bedd7ddb3f7394ae32950a9415b16d3e9', 'title': 'Animate124: Animating One Image to 4D Dynamic Scene'}, {'paperId': '185e88645dfc07d6ca81a55dfc66bd3452400276', 'title': 'ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs'}, {'paperId': 'f673d3a0f98094190c5ed1fe8e18673803f1d38c', 'title': 'LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes'}, {'paperId': '85b10400864187230714506412c85610c786b5c3', 'title': 'Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning'}, {'paperId': '9b86ce1bde87b304141641b49299f4d0f1f7ba1d', 'title': 'I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models'}, {'paperId': '083bab4a967c2221d9f4da9110fe37d8ca679078', 'title': 'DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors'}, {'paperId': '395bfdae59f1a5fde16213cade43d2587e4565df', 'title': '4D Gaussian Splatting for Real-Time Dynamic Scene Rendering'}, {'paperId': '6541f0f74cf6f6d0d8b4a8d9efb64d5e0729bc13', 'title': 'MotionDirector: Motion Customization of Text-to-Video Diffusion Models'}, {'paperId': '4584dee8505ce8cdaa09d7c3f4b4ab6568b3e766', 'title': 'RealFill: Reference-Driven Generation for Authentic Image Completion'}, {'paperId': 'a5b7fc1bff0910ff31975ec0a15ed30c41f0a968', 'title': 'Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation'}, {'paperId': '2854e5bab8e6f36e54c64456628a9559bf67019e', 'title': 'IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models'}, {'paperId': '276f6117b8b8549a47461653b95e657278260ee3', 'title': 'HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models'}, {'paperId': 'c1caa303549764d220ff17dc1785985dd1ba6047', 'title': 'AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning'}, {'paperId': '861370f7c2d18bed09905fde334a19cc96e83e14', 'title': 'StyleDrop: Text-to-Image Generation in Any Style'}, {'paperId': 'c5e9fd131cde68c218d0ea69cd617a67c7f35d42', 'title': 'ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation'}, {'paperId': '99580c77ff979fc97d20a3b044536bb8567d24e4', 'title': 'HOSNeRF: Dynamic Human-Object-Scene Neural Radiance Fields from a Single Video'}, {'paperId': 'f5a0c57f90c6abe31482e9f320ccac5ee789b135', 'title': 'Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models'}, {'paperId': '83b8e18488d8f31dd017ec0b26531cef4b635b36', 'title': 'Subject-driven Text-to-Image Generation via Apprenticeship Learning'}, {'paperId': '0cbb518c364067200476a51e5ce7476a4f582770', 'title': 'Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation'}, {'paperId': '26c22380282a00166273038bc5ba785d845d61ad', 'title': 'Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions'}, {'paperId': 'd36a632f95b45dc072fa0cbac7e494b758e1146a', 'title': 'Human Pose as Compositional Tokens'}, {'paperId': '95aa6fa4e42387561cff22378348d528adea37f2', 'title': 'Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models'}, {'paperId': '2c70684973bc4d7b6f8404a647b8031c4d3c8383', 'title': 'Zero-1-to-3: Zero-shot One Image to 3D Object'}, {'paperId': '14ccb8bcceb6de10eda6ad08bec242a4f2946497', 'title': 'FateZero: Fusing Attentions for Zero-shot Text-based Video Editing'}, {'paperId': 'ebe2a5a186fa08e4e1b5d225d7a253d444d2c396', 'title': 'ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth'}, {'paperId': '9758ddd6ffbaac75aa0447a9664e6989811a05e2', 'title': 'Dreamix: Video Diffusion Models are General Video Editors'}, {'paperId': 'd684fbe07585be651cc93d3c00ae3fe6df3ac877', 'title': 'Text-To-4D Dynamic Scene Generation'}, {'paperId': 'a6c3dce03b16c366ba28dfa2232fb68d562e5474', 'title': 'HexPlane: A Fast Representation for Dynamic Scenes'}, {'paperId': '5750680aca638c3f90a84f45902e1ef3135c0e98', 'title': 'Robust Dynamic Radiance Fields'}, {'paperId': '1367dcff4ccb927a5e95c452041288b3f0dd0eff', 'title': 'Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation'}, {'paperId': '736973165f98105fec3729b7db414ae4d80fcbeb', 'title': 'Scalable Diffusion Models with Transformers'}, {'paperId': '3d94322b049959cac15efd67af22207b73afa245', 'title': 'MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation'}, {'paperId': 'fc011ed5ee986332523a62d2783adee1179dc1ed', 'title': 'Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation'}, {'paperId': 'cb4b91a6a7103410c1bcde9df69ab641c2e79a10', 'title': 'Latent Video Diffusion Models for High-Fidelity Long Video Generation'}, {'paperId': '7129623909b2a944f0d486bf2b9dd7e242552b83', 'title': 'DynIBaR: Neural Dynamic Image-Based Rendering'}, {'paperId': 'bdf4af8311637c681904e71cf50f96fd0026f578', 'title': 'Magic3D: High-Resolution Text-to-3D Content Creation'}, {'paperId': '09179234dec5fc6dea4a8b02a9d98eef712d45e9', 'title': 'Monocular Dynamic View Synthesis: A Reality Check'}, {'paperId': '498ac9b2e494601d20a3d0211c16acf2b7954a54', 'title': 'Imagen Video: High Definition Video Generation with Diffusion Models'}, {'paperId': '1e33716e8820b867d5a8aaebab44c2d3135ea4ac', 'title': 'Make-A-Video: Text-to-Video Generation without Text-Video Data'}, {'paperId': '4c94d04afa4309ec2f06bdd0fe3781f91461b362', 'title': 'DreamFusion: Text-to-3D using 2D Diffusion'}, {'paperId': '5b19bf6c3f4b25cac96362c98b930cf4b37f6744', 'title': 'DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation'}, {'paperId': '5406129d9d7d00dc310671c43597101b0ee93629', 'title': 'An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion'}, {'paperId': '707bd332d2c21dc5eb1f02a52d4a0506199aae76', 'title': 'CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers'}, {'paperId': 'e3d06054af531ee2f42270d43100b309c28546ef', 'title': 'MUSIQ: Multi-scale Image Quality Transformer'}, {'paperId': 'f671a09e3e5922e6d38cb77dda8d76d5ceac2a27', 'title': 'SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations'}, {'paperId': 'a8ca46b171467ceb2d7652fbfb67fe701ad86092', 'title': 'LoRA: Low-Rank Adaptation of Large Language Models'}, {'paperId': 'ad4a0938c48e61b7827869e4ac3baffd0aefab35', 'title': 'Emerging Properties in Self-Supervised Vision Transformers'}, {'paperId': 'd67720b062f5dde1de3298d3140b7bbd332003cd', 'title': 'Neural Radiance Flow for 4D View Synthesis and Video Processing'}, {'paperId': '694bdf6e5906992dad2987a3cc8d1a176de691c9', 'title': 'D-NeRF: Neural Radiance Fields for Dynamic Scenes'}, {'paperId': '13034a395d5c6728c9b11e777828d9998018cbf6', 'title': 'Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes'}, {'paperId': '0f1af3f94f4699cd70a554f68f8f9e2c8e3d53dd', 'title': 'Nerfies: Deformable Neural Radiance Fields'}, {'paperId': '414da5e0716eb4707e033b7967d7671dfe71ab71', 'title': 'Space-time Neural Irradiance Fields for Free-Viewpoint Video'}, {'paperId': '014576b866078524286802b1d0e18628520aa886', 'title': 'Denoising Diffusion Implicit Models'}, {'paperId': '5c126ae3421f05768d8edd97ecd44b1364e2c99a', 'title': 'Denoising Diffusion Probabilistic Models'}, {'paperId': 'f1797507b0540026fea32ca0e2c50897524212e4', 'title': '4D Visualization of Dynamic Events From Unconstrained Multi-View Videos'}, {'paperId': 'c5cf1031bb6fbddab1add841096461e413bb69fc', 'title': 'ZeroNVS: Zero-Shot 360-Degree View Synthesis from a Single Real Image'}, {'paperId': '735fcf085059f419112b76f7217e7f1407efcbb0', 'title': 'Latent Video Diffusion Models for High-Fidelity Video Generation with Arbitrary Lengths'}, {'paperId': '8afb9c3abc3ef6335c3a14d46cf37dbe27462c8c', 'title': 'X-Fields: Implicit Neural View-, Light-and Time-Image Interpolation'}, {'paperId': None, 'title': '4d radiance fields with multi-scale occupancy networks for dynamic scene reconstruction'}, {'paperId': None, 'title': 'Sv3d: Scalable video 3d generation'}, {'paperId': None, 'title': 'Neural 3d video synthesis from multi-view video'}, {'paperId': None, 'title': 'Scenewiz3d: Interactive 3d scene generation with scene graphs'}, {'paperId': None, 'title': ': Efficient video'}, {'paperId': None, 'title': 'Nerf: Representing scenes as neural radiance fields for view synthesis'}]"
1947182893,Analyzing The Language of Visual Tokens,"David M. Chan, Rodolfo Corona, Joonyong Park, Cheol Jun Cho, Yutong Bai, Trevor Darrell",2024-11-07 18:59:28+00:00,"With the introduction of transformer-based models for vision and language
tasks, such as LLaVA and Chameleon, there has been renewed interest in the
discrete tokenized representation of images. These models often treat image
patches as discrete tokens, analogous to words in natural language, learning
joint alignments between visual and human languages. However, little is known
about the statistical behavior of these visual languages - whether they follow
similar frequency distributions, grammatical structures, or topologies as
natural languages. In this paper, we take a natural-language-centric approach
to analyzing discrete visual languages and uncover striking similarities and
fundamental differences. We demonstrate that, although visual languages adhere
to Zipfian distributions, higher token innovation drives greater entropy and
lower compression, with tokens predominantly representing object parts,
indicating intermediate granularity. We also show that visual languages lack
cohesive grammatical structures, leading to higher perplexity and weaker
hierarchical organization compared to natural languages. Finally, we
demonstrate that, while vision models align more closely with natural languages
than other models, this alignment remains significantly weaker than the
cohesion found within natural languages. Through these experiments, we
demonstrate how understanding the statistical properties of discrete visual
languages can inform the design of more effective computer vision models.",http://arxiv.org/pdf/2411.05001v1,http://arxiv.org/abs/2411.05001v1,63,0,"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","{'model': 'tldr@v2.0.0', 'text': 'A natural-language-centric approach is taken to analyzing discrete visual languages and it is demonstrated that, although visual languages adhere to Zipfian distributions, higher token innovation drives greater entropy and lower compression, with tokens predominantly representing object parts, indicating intermediate granularity.'}",[],"[{'paperId': '5b3991fe7d8f6fc0a7fbd42938e2988ea37efe14', 'title': 'Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model'}, {'paperId': '88fc09d3cd7bda6d3e5008b2a691816191cbfe51', 'title': 'mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models'}, {'paperId': 'e8663f61ac3dbe7c1845ea125e3d3f1b2c6dc9ef', 'title': 'SPIN: Hierarchical Segmentation with Subpart Granularity in Natural Images'}, {'paperId': '94773f22b5befd0e167a7de525d29bec2b09937a', 'title': 'Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs'}, {'paperId': 'b15e6e2b1d81bc110f8fc98c3caf2e25e2512539', 'title': 'Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation'}, {'paperId': '32112b798f70faab00e14806f51d46058cf5e597', 'title': 'Chameleon: Mixed-Modal Early-Fusion Foundation Models'}, {'paperId': '689c358c5f9b5b1693a8bcc7e6e0460012f5cf9e', 'title': 'Sequential Modeling Enables Scalable Learning for Large Vision Models'}, {'paperId': '2bb68bba87c6212b423b6b2ad36e516363ce8a0b', 'title': 'Benford’s Law applies to word frequency rank in English, German, French, Spanish, and Italian'}, {'paperId': 'a5036f31f0e629dc661f120b8c3b1f374d479ab8', 'title': 'Visual Instruction Tuning'}, {'paperId': '52fa59615baf31e1085450cd31d1c7c23968dbd0', 'title': 'Unsupervised Discontinuous Constituency Parsing with Mildly Context-Sensitive Grammars'}, {'paperId': 'a8260077135246476a0b0601495ef08e56c21a50', 'title': 'Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset'}, {'paperId': 'dfa2c99fecaf0f5a592af5e63a4e51b76eaee2c6', 'title': 'What’s in a Caption? Dataset-Specific Linguistic Diversity and Its Effect on Visual Description Models and Metrics'}, {'paperId': 'c57293882b2561e1ba03017902df9fc2f289dea2', 'title': 'Hierarchical Text-Conditional Image Generation with CLIP Latents'}, {'paperId': '15e234a67f30d6761f1d7670d501095d1697b69c', 'title': 'Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors'}, {'paperId': 'c10075b3746a9f3dd5811970e93c8ca3ad39b39d', 'title': 'High-Resolution Image Synthesis with Latent Diffusion Models'}, {'paperId': '414e554d281d529401c873cb9c97186365ec5dd8', 'title': 'Vector Quantized Diffusion Model for Text-to-Image Synthesis'}, {'paperId': '601ab36b6f077ff57472f4a0cf2e061dd05b9b85', 'title': 'Discrete Representations Strengthen Vision Transformer Robustness'}, {'paperId': 'b69fb6e4d4b62b61ae224106ff1094ece2c813b6', 'title': 'Hubert: How Much Can a Bad Teacher Benefit ASR Pre-Training?'}, {'paperId': '38b0567e83386ddc294d6c81b541deacbd8e3c2a', 'title': 'CLIPScore: A Reference-free Evaluation Metric for Image Captioning'}, {'paperId': 'ad7ddcc14984caae308c397f1a589aae75d4ab71', 'title': 'Training data-efficient image transformers & distillation through attention'}, {'paperId': '47f7ec3d0a5e6e83b6768ece35206a94dc81919c', 'title': 'Taming Transformers for High-Resolution Image Synthesis'}, {'paperId': '2e4ca3d95ffb83870661dd66deee143e782f0706', 'title': 'Curious Case of Language Generation Evaluation Metrics: A Cautionary Tale'}, {'paperId': 'c1e5c4d8c52e17ece0bbb4e8b564e8b903a20e39', 'title': 'Visually Grounded Compound PCFGs'}, {'paperId': '7ea59779ffb392f099d5304680126b4299f43750', 'title': 'Compound Probabilistic Context-Free Grammars for Grammar Induction'}, {'paperId': '6be216d93421bf19c1659e7721241ae73d483baf', 'title': 'Generating Diverse High-Fidelity Images with VQ-VAE-2'}, {'paperId': '57c13015444162846b8c9326d3a8be83d3712610', 'title': 'Language Modeling at Scale'}, {'paperId': '86c39502467fd2f482b95e178bf4dfd2978e4a32', 'title': ""Zipf's law in 50 languages: its structural pattern, linguistic interpretation, and cognitive motivation""}, {'paperId': 'b4df354db88a70183a64dbc9e56cf14e7669a6c0', 'title': 'Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning'}, {'paperId': '928f9dccb806a3278d20d82cc53781c5f44e2bb1', 'title': 'Constituency Parsing with a Self-Attentive Encoder'}, {'paperId': 'f466157848d1a7772fb6d02cdac9a7a5e7ef982e', 'title': 'Neural Discrete Representation Learning'}, {'paperId': '70c46bb2599c40aaef9baaf112a0d0091c6f8685', 'title': 'Zipf’s and Benford’s laws in Twitter hashtags'}, {'paperId': '618caee6385f0a98ff094abe8bedddde73235303', 'title': 'Word2Vec'}, {'paperId': 'f90d9c5615f4a0e3f9a1ce2a0075269b9bab6b5f', 'title': 'SPICE: Semantic Propositional Image Caption Evaluation'}, {'paperId': 'f37e1b62a767a307c046404ca96bc140b3e68cb5', 'title': 'GloVe: Global Vectors for Word Representation'}, {'paperId': 'e74f9b7f8eec6ba4704c206b93bc8079af3da4bd', 'title': 'ImageNet Large Scale Visual Recognition Challenge'}, {'paperId': '71b7178df5d2b112d07e45038cb5637208659ff7', 'title': 'Microsoft COCO: Common Objects in Context'}, {'paperId': 'b387a36c3cbf05f5671670471985d3f0985795bd', 'title': 'Selected Studies of the Principle of Relative Frequency in Language'}, {'paperId': '410e56d58adb95bd2fb6cf6b259e43c621d78d11', 'title': 'powerlaw: A Python Package for Analysis of Heavy-Tailed Distributions'}, {'paperId': '4a89dec398829629219030b9fa7988e273a428fb', 'title': ""Benford's law in the natural sciences""}, {'paperId': 'c9d61b204f91bcad06f6dd9c79ea25bf53747c4c', 'title': 'Benford’s Law as an Indicator of Fraud in Economics'}, {'paperId': 'd2c733e34d48784a37d717fe43d9e93277a8c53e', 'title': 'ImageNet: A large-scale hierarchical image database'}, {'paperId': '9cbe962b523d6e2cd252dbc900890af9e15b1009', 'title': ""Zipf and Heaps Laws' Coefficients Depend on Language""}, {'paperId': '2db86557aa1aaca3bfabe419b2abd949f42e08be', 'title': 'Quantitative tools for comparing animal communication systems: information theory applied to bottlenose dolphin whistle repertoires'}, {'paperId': 'be8cdeafb6bcda28468fec7733cedc3259dbdf83', 'title': 'Origins of scaling in natural images'}, {'paperId': '8f313640d24a3fbc5638c2bc2123faf2b0ff080e', 'title': 'Hausdorff dimension of quasi-circles'}, {'paperId': '6c79a9bb8f885050cad70b4c69e016b186ffa538', 'title': 'Trainable grammars for speech recognition'}, {'paperId': 'b9855a8dc77f6c28d56787136d68ad4f68bbad99', 'title': 'Generalized procrustes analysis'}, {'paperId': '2aad2211fa65b0f0306b198d6b5a3e392cd6531d', 'title': 'ON A CLASS OF SKEW DISTRIBUTION FUNCTIONS'}, {'paperId': 'a068df4c5f829bd0b85bf72fd919006c563c6345', 'title': 'A method for the construction of minimum-redundancy codes'}, {'paperId': '6b554229ff77f9e3ee66d909acc3bbf3d594199e', 'title': 'Some Statistics of Evolution and Geographical Distribution in Plants and Animals, and their Significance.'}, {'paperId': '0e12448355c3bc34f2a4f8fc31c2b2ca8609095f', 'title': 'Re-evaluating the Need for Visual Signals in Unsupervised Grammar Induction'}, {'paperId': 'ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f', 'title': 'AUTO-ENCODING VARIATIONAL BAYES'}, {'paperId': None, 'title': 'Benford’s law in linguistic texts: Its principle and applications'}, {'paperId': '5ac5b7b68f49f963517bef352bd386932d550e09', 'title': ""Zipf's Law in Image Coding Schemes""}, {'paperId': None, 'title': 'Gustav'}, {'paperId': 'ed1fbc8829224a446bcd79f324cc71579e3794fe', 'title': 'The Algebraic Theory of Context-Free Languages*'}, {'paperId': '2c0cdd08a8d3cb834da33515037eafa4bdb5faff', 'title': 'An Empirical Bayes Approach to Statistics'}, {'paperId': None, 'title': 'Contribution à la théorie mathématique des jeux de communication'}, {'paperId': 'c1e3f2d537e50e0d5263e4731ab6c7983acd6687', 'title': 'Prediction and Entropy of Printed English'}, {'paperId': None, 'title': 'The law of anomalous numbers'}, {'paperId': 'ec896a712a0bbf40c9f47e7f316ba886c2e4b9ec', 'title': 'Relativ frequency of English speech sounds'}, {'paperId': None, 'title': 'A,B )=max a ∈ A min b ∈ B ∥ a − b ∥ where A and B are two point sets, and ∥ a − b ∥ is the Euclidean distance between point a ∈ Similar to Figure J.2, for simplicity and clarity'}, {'paperId': None, 'title': 'The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale'}]"
3243772107,DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation,"Peiqi Liu, Zhanqiu Guo, Mohit Warke, Soumith Chintala, Chris Paxton, Nur Muhammad Mahi Shafiullah, Lerrel Pinto",2024-11-07 18:59:27+00:00,"Significant progress has been made in open-vocabulary mobile manipulation,
where the goal is for a robot to perform tasks in any environment given a
natural language description. However, most current systems assume a static
environment, which limits the system's applicability in real-world scenarios
where environments frequently change due to human intervention or the robot's
own actions. In this work, we present DynaMem, a new approach to open-world
mobile manipulation that uses a dynamic spatio-semantic memory to represent a
robot's environment. DynaMem constructs a 3D data structure to maintain a
dynamic memory of point clouds, and answers open-vocabulary object localization
queries using multimodal LLMs or open-vocabulary features generated by
state-of-the-art vision-language models. Powered by DynaMem, our robots can
explore novel environments, search for objects not found in memory, and
continuously update the memory as objects move, appear, or disappear in the
scene. We run extensive experiments on the Stretch SE3 robots in three real and
nine offline scenes, and achieve an average pick-and-drop success rate of 70%
on non-stationary objects, which is more than a 2x improvement over
state-of-the-art static systems. Our code as well as our experiment and
deployment videos are open sourced and can be found on our project website:
https://dynamem.github.io/",http://arxiv.org/pdf/2411.04999v1,http://arxiv.org/abs/2411.04999v1,0,0,"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Engineering', 'source': 's2-fos-model'}]","{'model': 'tldr@v2.0.0', 'text': ""DynaMem is presented, a new approach to open-world mobile manipulation that uses a dynamic spatio-semantic memory to represent a robot's environment and answers open-vocabulary object localization queries using multimodal LLMs or open-vocabulary features generated by state-of-the-art vision-language models.""}",[],[]
3418760641,LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation,"Weiquan Huang, Aoqi Wu, Yifan Yang, Xufang Luo, Yuqing Yang, Liang Hu, Qi Dai, Xiyang Dai, Dongdong Chen, Chong Luo, Lili Qiu",2024-11-07 18:59:16+00:00,"CLIP is one of the most important multimodal foundational models today. What
powers CLIP's capabilities? The rich supervision signals provided by natural
language, the carrier of human knowledge, shape a powerful cross-modal
representation space. However, with the rapid advancements in large language
models LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and
generation are continually being pushed. This raises an intriguing question:
can the capabilities of LLMs be harnessed to further improve multimodal
representation learning? The potential benefits of incorporating LLMs into CLIP
are clear. LLMs' strong textual understanding can fundamentally improve CLIP's
ability to handle image captions, drastically enhancing its ability to process
long and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs
are trained on a vast corpus of text, possessing open-world knowledge. This
allows them to expand on caption information during training, increasing the
efficiency of the learning process. In this paper, we propose LLM2CLIP, a novel
approach that embraces the power of LLMs to unlock CLIP's potential. By
fine-tuning the LLM in the caption space with contrastive learning, we extract
its textual capabilities into the output embeddings, significantly improving
the output layer's textual discriminability. We then design an efficient
training process where the fine-tuned LLM acts as a powerful teacher for CLIP's
visual encoder. Thanks to the LLM's presence, we can now incorporate longer and
more complex captions without being restricted by vanilla CLIP's text encoder's
context window and ability limitations. Our experiments demonstrate that this
approach brings substantial improvements in cross-modal tasks.",http://arxiv.org/pdf/2411.04997v1,http://arxiv.org/abs/2411.04997v1,47,0,"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","{'model': 'tldr@v2.0.0', 'text': ""This paper proposes LLM2CLIP, a novel approach that embraces the power of LLMs to unlock CLIP's potential by fine-tuning the LLM in the caption space with contrastive learning, and extracting its textual capabilities into the output embeddings, significantly improving the output layer's textual discriminability.""}",[],"[{'paperId': '6b598461002765d3742bee4e50521010e444fc10', 'title': 'Online Zero-Shot Classification with CLIP'}, {'paperId': '6520557cc3bfd198f960cc8cb6151c3474321bd8', 'title': 'The Llama 3 Herd of Models'}, {'paperId': '68f0f70e3779003de6c946daf4db9a19408b9b6f', 'title': 'Advancing Multi-Modal Sensing Through Expandable Modality Alignment'}, {'paperId': 'cd5d736218589e0d9a8db4808f31eddcc038a3cb', 'title': 'E5-V: Universal Embeddings with Multimodal Large Language Models'}, {'paperId': 'e9883986681458534f408ee30192ba98f5708ce5', 'title': 'MATE: Meet At The Embedding - Connecting Images with Long Texts'}, {'paperId': '94773f22b5befd0e167a7de525d29bec2b09937a', 'title': 'Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs'}, {'paperId': 'fe057be397c9cc9fe56fb19b88830d596bd99ba8', 'title': 'CLIP-Branches: Interactive Fine-Tuning for Text-Image Retrieval'}, {'paperId': '82bc594ddf77fe8e69e9b41dc32960d7f16b4b1d', 'title': 'What If We Recaption Billions of Web Images with LLaMA-3?'}, {'paperId': '83e89d4e09bbcda9242fe5d888375cfbf7356659', 'title': 'Jina CLIP: Your CLIP Model Is Also Your Text Retriever'}, {'paperId': '4450c251dfcb8ca2cede4c135578efa0d769b8f9', 'title': 'DOCCI: Descriptions of Connected and Contrasting Images'}, {'paperId': '18594d3f15b39385bace39657d14f0c8c7479db1', 'title': 'LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders'}, {'paperId': '0f284b2fdf001ced671ef87bea3435849c1e8059', 'title': 'DreamLIP: Language-Image Pre-training with Long Captions'}, {'paperId': '4956e8f57227ec8f642dc54e2c0c3742ebb388e7', 'title': 'Long-CLIP: Unlocking the Long-Text Capability of CLIP'}, {'paperId': 'a3c4047f82d7120e2bbc600dbe79b930e1dc4e41', 'title': 'Repetition Improves Language Model Embeddings'}, {'paperId': '3c5fa29b3c1c55cfd7a4ed87a4a484dd6a488eb2', 'title': 'Data-Efficient Multimodal Fusion on a Single GPU'}, {'paperId': 'a17fe25540a96782cd1f24d7be512f7516359a7f', 'title': 'A Picture is Worth More Than 77 Text Tokens: Evaluating CLIP-Style Models on Dense Captions'}, {'paperId': '1206b05eae5a06ba662ae79fb291b50e359c4f42', 'title': 'Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets'}, {'paperId': 'c91905e7fbf1b825f19374c854cea16613f3c652', 'title': 'Unified Medical Image Pre-training in Language-Guided Common Semantic Space'}, {'paperId': 'f68f6f2a057c4e6e5a3c91fc8563533d9bf6e560', 'title': 'ShareGPT4V: Improving Large Multi-Modal Models with Better Captions'}, {'paperId': 'e22ae34ea102a781d0494e115639e8d081bf6920', 'title': 'Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents'}, {'paperId': 'd4af12327385260116dfd68ed1ec6d0602d26d1f', 'title': 'Improving CLIP Training with Language Rewrites'}, {'paperId': 'a08b7123a7158f1a7fbbc18e8b5aaebd47980ecf', 'title': 'EVA-CLIP: Improved Training Techniques for CLIP at Scale'}, {'paperId': '3049c992adbd56e29c4d957ee0c4e9d05fe3c6d1', 'title': 'EVA-02: A Visual Representation for Neon Genesis'}, {'paperId': '10cd235fb975aa958d9ce318f110606d5631fefe', 'title': 'GridCLIP: One-Stage Object Detection by Grid-Level CLIP Representation Learning'}, {'paperId': '163b4d6a79a5b19af88b8585456363340d9efd04', 'title': 'GPT-4 Technical Report'}, {'paperId': 'efbe97d20c4ffe356e8826c01dc550bacc405add', 'title': 'Adding Conditional Control to Text-to-Image Diffusion Models'}, {'paperId': '67db43cb6cc618c873c63fe2c83025c335b7a230', 'title': 'ZegCLIP: Towards Adapting CLIP for Zero-shot Semantic Segmentation'}, {'paperId': '3c2b12824b0027edb49b68300cbeab02cfc49ca8', 'title': 'Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese'}, {'paperId': 'a5c4e494b0cfa75589b0d462e049d058eb81f754', 'title': 'IMU2CLIP: Multimodal Contrastive Learning for IMU Motion Sensors from Egocentric Videos and Text'}, {'paperId': 'e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9', 'title': 'LAION-5B: An open large-scale dataset for training next generation image-text models'}, {'paperId': 'cd2f5bc7f6e21b4f6f127465de8b16ab19aca437', 'title': 'APE: Aligning Pretrained Encoders to Quickly Learn Aligned Multimodal Representations'}, {'paperId': None, 'title': 'ProtoCLIP: Prototypical Contrastive Language Image Pretraining'}, {'paperId': 'c57293882b2561e1ba03017902df9fc2f289dea2', 'title': 'Hierarchical Text-Conditional Image Generation with CLIP Latents'}, {'paperId': 'fe0e647ac5bbe9b127caddfcb52b9f723d6f158c', 'title': 'Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark'}, {'paperId': 'c10075b3746a9f3dd5811970e93c8ca3ad39b39d', 'title': 'High-Resolution Image Synthesis with Latent Diffusion Models'}, {'paperId': 'f3ce9ba3fcec362b70263a7ed63d9404975496a0', 'title': 'PointCLIP: Point Cloud Understanding by CLIP'}, {'paperId': '38b0567e83386ddc294d6c81b541deacbd8e3c2a', 'title': 'CLIPScore: A Reference-free Evaluation Metric for Image Captioning'}, {'paperId': 'c26759e6c701201af2f62f7ee4eb68742b5bf085', 'title': 'SimCSE: Simple Contrastive Learning of Sentence Embeddings'}, {'paperId': '6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4', 'title': 'Learning Transferable Visual Models From Natural Language Supervision'}, {'paperId': 'b4df354db88a70183a64dbc9e56cf14e7669a6c0', 'title': 'Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning'}, {'paperId': 'b795675a6228abb68f8ed1b8abaf8630309fd764', 'title': 'COCO-CN for Cross-Lingual Image Tagging, Captioning, and Retrieval'}, {'paperId': '39f3f9d22a072d0ccf423aa31bacbb4071ac0644', 'title': 'Fluency-Guided Cross-Lingual Image Captioning'}, {'paperId': 'efbd381493bb9636f489b965a2034d529cd56bcd', 'title': 'Pointer Sentinel Mixture Models'}, {'paperId': '8e080b98efbe65c02a116439205ca2344b9f7cd4', 'title': 'Im2Text: Describing Images Using 1 Million Captioned Photographs'}, {'paperId': '5ddb51ae85deca14dc7fc8adc07305c22a1ebe0a', 'title': 'Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities'}, {'paperId': 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'}, {'paperId': None, 'title': 'Llava-next: Stronger llms supercharge multimodal capabilities in the wild, May 2024a'}]"
3231343356,HourVideo: 1-Hour Video-Language Understanding,"Keshigeyan Chandrasegaran, Agrim Gupta, Lea M. Hadzic, Taran Kota, Jimming He, Cristóbal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, Li Fei-Fei",2024-11-07 18:59:16+00:00,"We present HourVideo, a benchmark dataset for hour-long video-language
understanding. Our dataset consists of a novel task suite comprising
summarization, perception (recall, tracking), visual reasoning (spatial,
temporal, predictive, causal, counterfactual), and navigation (room-to-room,
object retrieval) tasks. HourVideo includes 500 manually curated egocentric
videos from the Ego4D dataset, spanning durations of 20 to 120 minutes, and
features 12,976 high-quality, five-way multiple-choice questions. Benchmarking
results reveal that multimodal models, including GPT-4 and LLaVA-NeXT, achieve
marginal improvements over random chance. In stark contrast, human experts
significantly outperform the state-of-the-art long-context multimodal model,
Gemini Pro 1.5 (85.0% vs. 37.3%), highlighting a substantial gap in multimodal
capabilities. Our benchmark, evaluation toolkit, prompts, and documentation are
available at https://hourvideo.stanford.edu",http://arxiv.org/pdf/2411.04998v1,http://arxiv.org/abs/2411.04998v1,96,0,"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Linguistics', 'source': 's2-fos-model'}]","{'model': 'tldr@v2.0.0', 'text': 'HourVideo, a benchmark dataset for hour-long video-language understanding, consists of a novel task suite comprising summarization, perception, perception, visual reasoning, visual reasoning, and navigation tasks, and navigation tasks.'}",[],"[{'paperId': '4b39e96841fbd0dca070d4172228ff15f22e9424', 'title': 'Tarsier: Recipes for Training and Evaluating Large Video Description Models'}, {'paperId': '920731b589af90a5b79236f4939ac117bbb939f2', 'title': 'OpenEQA: Embodied Question Answering in the Era of Foundation Models'}, {'paperId': '039c0f0b4142823fe214ddf2470230e211c7b8ee', 'title': 'STAR: A Benchmark for Situated Reasoning in Real-World Videos'}, {'paperId': '9d29da83aba362c728c36f4dea9dde678ae3e2b2', 'title': 'PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning'}, {'paperId': '4b445a06b615ca215951b2d7176d9de09124cdde', 'title': 'Pegasus-v1 Technical Report'}, {'paperId': 'beaa06656a9761e7860b0fe4eeb10800bd751320', 'title': 'From Image to Video, what do we need in multimodal LLMs?'}, {'paperId': 'f32b5b59fc4a988c49b112e4d4a06d684d4f117a', 'title': 'MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding'}, {'paperId': 'c7453d3e481cd419ec103af45aed2a76ac4d55e3', 'title': 'LongVLM: Efficient Long Video Understanding via Large Language Models'}, {'paperId': '6d017adda6b2b1ea627dde2f0e85401ebb9fe566', 'title': 'MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?'}, {'paperId': '0fce243964da0ec358152f226b21432e5a658917', 'title': 'Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context'}, {'paperId': 'ac8089bb7944090cf1de5df25aadf5e6356f3040', 'title': 'TempCompass: Do Video LLMs Really Understand Videos?'}, {'paperId': '56994972adca9319577617345128e46803a4043f', 'title': 'Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models'}, {'paperId': '8eae862d9669e7001eeee17b49fba793df9672c4', 'title': 'Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers'}, {'paperId': '4b48fa99d17e130da58b0f72c2c4b50f72d2e146', 'title': 'Video ReCap: Recursive Captioning of Hour-Long Videos'}, {'paperId': '3c23f28bac6c9387573a645673622172ea8b50a5', 'title': 'Memory Consolidation Enables Long-Context Video Understanding'}, {'paperId': 'bce43cb9af37a0c8d90f8cadaebd6bb002685edd', 'title': 'InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model'}, {'paperId': '4641fe56cd44144b6cabea583233ed952f97f4c0', 'title': 'A Simple LLM Framework for Long-Range Video Question-Answering'}, {'paperId': '53036b0c70eee0a7e211702b6b2f6c49a4c4f925', 'title': 'A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise'}, {'paperId': 'eca8a3e6383e3618e0bc984382e08c09be3cca6c', 'title': 'TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding'}, {'paperId': '17ff6a0844afe74796022e7aaf372553e9303d72', 'title': 'VTimeLLM: Empower LLM to Grasp Video Moments'}, {'paperId': '147c30c6636e780e3729198dd7fbc09d981e4d3c', 'title': 'Betrayed by Attention: A Simple yet Effective Approach for Self-supervised Video Object Segmentation'}, {'paperId': '679105b4343f316cab0c2e1ce3be0ed498341b86', 'title': 'VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models'}, {'paperId': 'ea3448eb86a233189631d914721e587d45931b64', 'title': 'MVBench: A Comprehensive Multi-modal Video Understanding Benchmark'}, {'paperId': 'b50d19c5c298f6562c3b3c6c3822a351bdc89260', 'title': 'MMMU: A Massive Multi-Discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI'}, {'paperId': 'b037bb09aa162d8a543e64ec777ca0edc732d2af', 'title': 'Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models'}, {'paperId': 'cb76f7fc35ff289fdf1cf50d9cfe1493342a0dec', 'title': 'AutoEval-Video: An Automatic Benchmark for Assessing Large Vision Language Models in Open-Ended Video Question Answering'}, {'paperId': '107fb6eec2febbae12db29bf3e311aaf5680027c', 'title': 'Video-LLaVA: Learning United Visual Representation by Alignment Before Projection'}, {'paperId': 'aad3d2e690f6c73f04a14622ceff51464bbc560e', 'title': 'Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding'}, {'paperId': '471bb71c8797c253fe668af5899dfc7fc2ddd8ac', 'title': 'Video-Teller: Enhancing Cross-Modal Generation with Fusion and Decoupling'}, {'paperId': 'c1e450284e7d6cac1855330a1197df8537df653f', 'title': 'InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition'}, {'paperId': 'f36c96827e629c86762bb96db70fab8f2660fc76', 'title': 'Can I Trust Your Answer? Visually Grounded Video Question Answering'}, {'paperId': 'b9540f7c1e27c2afd305942b7753ce741da91491', 'title': 'Are current long-term video understanding datasets long-term?'}, {'paperId': '0ee4fad1e29b82f0fba73cc657c5f41baa07e354', 'title': 'Semantics Meets Temporal Correspondence: Self-supervised Object-centric Learning in Videos'}, {'paperId': '656a6b3c0348d69cf9f98f95cbf68046941a4f29', 'title': 'EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding'}, {'paperId': '6c43305ffb387b08b274332e52710acef280a19a', 'title': 'Memory-and-Anticipation Transformer for Online Action Understanding'}, {'paperId': '94972e30504017156ef5b5debc419bf6edc67384', 'title': 'MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities'}, {'paperId': '7fbc502441d66daf1f53765d5d86a8dfba9ab0ce', 'title': 'OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models'}, {'paperId': '6f9b7c8cde1be2e62a503c31cac883c6d44c9d0d', 'title': 'MovieChat: From Dense Token to Sparse Memory for Long Video Understanding'}, {'paperId': '369b449415d50387fba048bbd4d26ee890df84b5', 'title': 'InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation'}, {'paperId': 'b37b1dc72b1882858f5120f2cd6883134089a6ed', 'title': 'MMBench: Is Your Multi-modal Model an All-around Player?'}, {'paperId': 'ea566f87f6253bb2d32cf7b61cd3e2535a0c3f42', 'title': 'mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding'}, {'paperId': '697e0add95e880bd42e00bef838181e105f91981', 'title': 'MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models'}, {'paperId': 'ebedc4d7a2356090904baba4104ef0832bc236df', 'title': 'A Survey on Multimodal Large Language Models'}, {'paperId': '4c4d176c6e28f48041f215d563f6ee8633534cff', 'title': 'Valley: Video Assistant with Large Language model Enhanced abilitY'}, {'paperId': 'bf7025a2e5dbb3c09deae02a1aa98a256ca559e2', 'title': 'Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models'}, {'paperId': '5d321194696f1f75cf9da045e6022b2f20ba5b9c', 'title': 'Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding'}, {'paperId': '9ef1ad3d452db449a04dbbe4ba9027a7059f897a', 'title': 'Relational Space-Time Query in Long-Form Videos'}, {'paperId': '42585728685d7b2567e62b03463d1520f1bbe47e', 'title': 'Perception Test: A Diagnostic Benchmark for Multimodal Video Models'}, {'paperId': '8badb0587fef2ffc078b0cec549eb8ec96ed3ad4', 'title': 'Self-Chained Image-Language Model for Video Localization and Question Answering'}, {'paperId': 'd48cb91b9e555194f7494c4d4bb9815021d3ee45', 'title': 'VideoChat: Chat-Centric Video Understanding'}, {'paperId': 'ca6a2bc279be5a3349a22bfd6866ed633d18734b', 'title': 'MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models'}, {'paperId': 'a5036f31f0e629dc661f120b8c3b1f374d479ab8', 'title': 'Visual Instruction Tuning'}, {'paperId': '163b4d6a79a5b19af88b8585456363340d9efd04', 'title': 'GPT-4 Technical Report'}, {'paperId': '3f5b31c4f7350dc88002c121aecbdc82f86eb5bb', 'title': 'BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models'}, {'paperId': 'b9129ae658a8bb833c11ff1646a895643397df62', 'title': 'Motion-inductive Self-supervised Object Discovery in Videos'}, {'paperId': 'c35ab79a2d105f542769bfa10a5534e03ab342c1', 'title': 'Static and Dynamic Concepts for Self-supervised Video Representation Learning'}, {'paperId': '01724c36660359545e1368fc80c99f4bde44a190', 'title': 'XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model'}, {'paperId': 'ad55721062b3073f67143d227cfe2a9bf4d74ea3', 'title': 'Dual Contrastive Learning for Spatio-temporal Representation'}, {'paperId': 'cb78447bf7d9ef8f10381fa22823e6424f148ba5', 'title': 'Robustness Analysis of Video-Language Models Against Visual and Language Perturbations'}, {'paperId': '809822d59203a462bc9f2e0f0e9a8314d6d469d4', 'title': 'Revisiting the “Video” in Video-Language Understanding'}, {'paperId': '26218bdcc3945c7edae7aa2adbfba4cd820a2df3', 'title': 'Flamingo: a Visual Language Model for Few-Shot Learning'}, {'paperId': 'ada81a4de88a6ce474df2e2446ad11fea480616e', 'title': 'Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language'}, {'paperId': '4990f7542f0600e0501a7e7a931b32eb7cb804d5', 'title': 'VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training'}, {'paperId': '1b6e810ce0afd0dd093f789d2b2742d047e316d5', 'title': 'Chain of Thought Prompting Elicits Reasoning in Large Language Models'}, {'paperId': 'a3b42a83669998f65df60d7c065a70d07ca95e99', 'title': 'BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation'}, {'paperId': 'b51fa25bf1ce7fd18737a36b72e80f8e5808973e', 'title': 'MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition'}, {'paperId': '848eb8367785910c2fe31372605954ad8f9dfe6c', 'title': 'Ego4D: Around the World in 3,000 Hours of Egocentric Video'}, {'paperId': 'a0c9910338cfe2916fa899c57adf41bf8eb215be', 'title': 'Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization'}, {'paperId': 'af86df6a0af3226a1b4b5eb27c17c9e45367f896', 'title': 'MultiBench: Multiscale Benchmarks for Multimodal Representation Learning'}, {'paperId': '70b4c724a0f22198f9a04f504b1b298299e4cc37', 'title': 'VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation'}, {'paperId': '1b937ff4b05e2b56c2c2fcdfa5baa3085cd5a08c', 'title': 'NExT-QA: Next Phase of Question-Answering to Explaining Temporal Actions'}, {'paperId': '6ed5a7fb19d0fae0b7c8b47b007db194e43c2592', 'title': 'A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning'}, {'paperId': 'bac87bdb1cabc35fafb8176a234d332ebcc02864', 'title': 'Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval'}, {'paperId': '6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4', 'title': 'Learning Transferable Visual Models From Natural Language Supervision'}, {'paperId': '141a5033d9994242b18bb3b217e79582f1ee9306', 'title': 'Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision'}, {'paperId': '0732df185bdfcb9c908ec30bb441252593f58875', 'title': 'MovieNet: A Holistic Dataset for Movie Understanding'}, {'paperId': '85b9e68eb27069e87181050035f40b79438dd220', 'title': 'A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark'}, {'paperId': '4f2c1af57c056102806a184517313804f66e7447', 'title': 'ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering'}, {'paperId': 'af1f7739283bdbd2b7a94903041f6d6afd991907', 'title': 'Towards VQA Models That Can Read'}, {'paperId': '842b24b04ef2b142d655c7b50cd6ab0835d89330', 'title': 'Video Object Segmentation Using Space-Time Memory Networks'}, {'paperId': 'a7ac99d7cf3f568ab1a741392144b646b856ae0c', 'title': 'GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering'}, {'paperId': '45cb2cbae5c0b4cc52e524cc191a2f8db674ed42', 'title': 'Long-Term Feature Banks for Detailed Video Understanding'}, {'paperId': 'e7e1313061b0d56364bd2c41f017deb954bb05db', 'title': 'TVQA: Localized, Compositional Video Question Answering'}, {'paperId': '451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c', 'title': 'GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding'}, {'paperId': '057b80e235b10799d03876ad25465208a4c64caf', 'title': 'Video Question Answering via Gradually Refined Attention over Appearance and Motion'}, {'paperId': 'b68811a9b5cafe4795a11c1048541750068b7ad0', 'title': 'The “Something Something” Video Database for Learning and Evaluating Visual Common Sense'}, {'paperId': '86e1bdbfd13b9ed137e4c4b8b459a3980eb257f6', 'title': 'The Kinetics Human Action Video Dataset'}, {'paperId': 'b2f521c02c6ed3080c5fe123e938cdf4555e6fd2', 'title': 'TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering'}, {'paperId': 'e10a5e0baf2aa87d804795af071808a9377cc80a', 'title': 'Towards Automatic Learning of Procedures From Web Instructional Videos'}, {'paperId': '97ad70a9fa3f99adf18030e5e38ebe3d90daa2db', 'title': 'VQA: Visual Question Answering'}, {'paperId': 'e74f9b7f8eec6ba4704c206b93bc8079af3da4bd', 'title': 'ImageNet Large Scale Visual Recognition Challenge'}, {'paperId': '71b7178df5d2b112d07e45038cb5637208659ff7', 'title': 'Microsoft COCO: Common Objects in Context'}, {'paperId': '27a3e6559f434110facc6972268577ff0d7f44fe', 'title': 'MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models'}, {'paperId': None, 'title': 'Llava-next: A strong zero-shot video understanding model'}, {'paperId': None, 'title': 'Key Tools Identiﬁcation:'}, {'paperId': None, 'title': 'For Type I, delve into the attributes of objects (color, shape etc)'}]"
3359830447,LoFi: Scalable Local Image Reconstruction with Implicit Neural Representation,"AmirEhsan Khorashadizadeh, Tobías I. Liaudat, Tianlin Liu, Jason D. McEwen, Ivan Dokmanić",2024-11-07 18:58:57+00:00,"Neural fields or implicit neural representations (INRs) have attracted
significant attention in machine learning and signal processing due to their
efficient continuous representation of images and 3D volumes. In this work, we
build on INRs and introduce a coordinate-based local processing framework for
solving imaging inverse problems, termed LoFi (Local Field). Unlike
conventional methods for image reconstruction, LoFi processes local information
at each coordinate \textit{separately} by multi-layer perceptrons (MLPs),
recovering the object at that specific coordinate. Similar to INRs, LoFi can
recover images at any continuous coordinate, enabling image reconstruction at
multiple resolutions. With comparable or better performance than standard CNNs
for image reconstruction, LoFi achieves excellent generalization to
out-of-distribution data and memory usage almost independent of image
resolution. Remarkably, training on $1024 \times 1024$ images requires just 3GB
of memory -- over 20 times less than the memory typically needed by standard
CNNs. Additionally, LoFi's local design allows it to train on extremely small
datasets with less than 10 samples, without overfitting or the need for
regularization or early stopping. Finally, we use LoFi as a denoising prior in
a plug-and-play framework for solving general inverse problems to benefit from
its continuous image representation and strong generalization. Although trained
on low-resolution images, LoFi can be used as a low-dimensional prior to solve
inverse problems at any resolution. We validate our framework across a variety
of imaging modalities, from low-dose computed tomography to radio
interferometric imaging.",http://arxiv.org/pdf/2411.04995v1,http://arxiv.org/abs/2411.04995v1,110,0,"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}, {'category': 'Engineering', 'source': 's2-fos-model'}]","{'model': 'tldr@v2.0.0', 'text': 'This work builds on INRs and introduces a coordinate-based local processing framework for solving imaging inverse problems, termed LoFi (Local Field), which achieves excellent generalization to out-of-distribution data and memory usage almost independent of image resolution.'}",[],"[{'paperId': 'ef3fc740d526ab8d5cf83667140edbb494f0a533', 'title': 'Latent Modulated Function for Computational Optimal Continuous Image Representation'}, {'paperId': 'e8018fb0497803e63bb56f5821036cc440f80ba1', 'title': 'Scalable Bayesian uncertainty quantification with data-driven priors for radio interferometric imaging'}, {'paperId': '2dfa7de1427e84341a53bd5051d966bc0e80d6e6', 'title': 'Patched Denoising Diffusion Models For High-Resolution Image Synthesis'}, {'paperId': '2f96d3d4f4a2b8a6ffcc1d8a8a85c3218004ae4f', 'title': 'Dynamic Implicit Image Function for Efficient Arbitrary-Scale Image Representation'}, {'paperId': '46943551a72ab1bf608067b63cb2668049b4b199', 'title': 'Denoising Diffusion Models for Plug-and-Play Image Restoration'}, {'paperId': '482ebb6f9c2b44d5181d807d161225ebff2c6e55', 'title': 'Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models'}, {'paperId': 'f0dd4f65886255ca00da65d56d5d8d173e2113a4', 'title': 'Deep Injective Prior for Inverse Scattering'}, {'paperId': 'dc444ee2a0beac5a6f0309c5119f4cc56a57dcdc', 'title': 'FunkNN: Neural Interpolation for Functional Generation'}, {'paperId': 'b5b157bbbbc5761b8ca08bf91ac0a077aaec3e66', 'title': 'Optimization for Amortized Inverse Problems'}, {'paperId': '61e46884567be7cad12e999365b16a8d3414b678', 'title': 'Diffusion Posterior Sampling for General Noisy Inverse Problems'}, {'paperId': 'a4b498d961a2bdb2eba1b9cbfb37f9e2b10d76e5', 'title': 'Implicit Neural Representation for Mesh-Free Inverse Obstacle Scattering'}, {'paperId': '75d4165fbacac1a26d12c83d3b9f8a2429798210', 'title': 'Conditional Injective Flows for Bayesian Imaging'}, {'paperId': '149042c8dd2e4c718c0d0ef411ec3799c15986d4', 'title': 'Plug-and-Play Methods for Integrating Physical and Learned Models in Computational Imaging: Theory, algorithms, and applications'}, {'paperId': '3d68ce250751337e1cbceac4969a30f27493480c', 'title': 'HUMUS-Net: Hybrid unrolled multi-scale network architecture for accelerated MRI reconstruction'}, {'paperId': '05a554ef3eb8e2114469f9d6a82405a74ea95ec9', 'title': 'Image-to-Image MLP-mixer for Image Reconstruction'}, {'paperId': '80ee4eb1b750d23261ae6d840ce5d24f69b33c83', 'title': 'From data to functa: Your data point is a function and you can treat it like one'}, {'paperId': '3d3c5fcbc40aadccceda58d3d9c5cd00588ea0b7', 'title': 'Denoising Diffusion Restoration Models'}, {'paperId': '3425495ee3b6ead009f35aeb70edeac4e6eb2d10', 'title': 'Patches Are All You Need?'}, {'paperId': '1e88d5afe19aea324d33541f60a90b7036894c32', 'title': 'Restormer: Efficient Transformer for High-Resolution Image Restoration'}, {'paperId': 'c5b05a4f5cb0e5ec8b2d295b09461daca680e9bd', 'title': 'Equivariant and Invariant Reynolds Networks'}, {'paperId': '3513f49b1d476ca5f86cf51e3d640e1afc5cb13b', 'title': 'Frame Averaging for Invariant and Equivariant Network Design'}, {'paperId': '608961bf3a24acf991c3c806212688d6d49a3c86', 'title': 'DDUNet: Dense Dense U-Net with Applications in Image Denoising'}, {'paperId': '810a27cb8cb491e759197c95460da27b53e3a98f', 'title': 'Wasserstein Patch Prior for Image Superresolution'}, {'paperId': '7a9a708ca61c14886aa0dcd6d13dac7879713f5f', 'title': 'SwinIR: Image Restoration Using Swin Transformer'}, {'paperId': 'a9194aa117acabe333aaaf330cf71c66094ced51', 'title': 'Robust Compressed Sensing MRI with Deep Generative Priors'}, {'paperId': '2835951fabf12804e17d5a525b2be2bee70e7910', 'title': 'Uformer: A General U-Shaped Transformer for Image Restoration'}, {'paperId': '64ea8f180d0682e6c18d1eb688afdb2027c02794', 'title': 'Diffusion Models Beat GANs on Image Synthesis'}, {'paperId': '3254d36e37535a3f6cba39dc04e0331453bcbb93', 'title': 'Truly Shift-Equivariant Convolutional Neural Networks with Adaptive Polyphase Upsampling'}, {'paperId': '67571d29190faea9fbd104acd16274f8c4edf254', 'title': 'MLP-Mixer: An all-MLP Architecture for Vision'}, {'paperId': '14014c024674991149f3ecf9314c93f7e029ef1a', 'title': 'Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges'}, {'paperId': 'e03b2940d653ab05a1364545f25c425610ff974d', 'title': 'LoDoPaB-CT, a benchmark dataset for low-dose computed tomography reconstruction'}, {'paperId': '40f4d7fe800810288a80f84cdb357a8f4c28e880', 'title': 'Rethinking Spatial Dimensions of Vision Transformers'}, {'paperId': '3e398bad2d8636491a1034cc938a5e024c7aa881', 'title': 'Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions'}, {'paperId': 'd98e14847a2ffdb6119890e2d6a33e953d10d995', 'title': 'Trumpets: Injective Flows for Inference and Inverse Problems'}, {'paperId': '1d41857ba8e0e8286268989036cd138a1f8ca372', 'title': 'Generative Models as Distributions of Functions'}, {'paperId': '767a6054796e2e6c1de453afab0e05e55aadf825', 'title': 'Learning Continuous Image Representation with Local Implicit Image Function'}, {'paperId': '6f6f73e69ee0d9d5d7d088bb882db1851d98175a', 'title': 'Pre-Trained Image Processing Transformer'}, {'paperId': 'adcb8cad96a2f73e0a2ef8b7ed1737af804de30b', 'title': 'Truly shift-invariant convolutional neural networks'}, {'paperId': 'bd57bf47d2cbc32fbd7fe0c15cf33700c1414039', 'title': 'Learning Multiscale Convolutional Dictionaries for Image Reconstruction'}, {'paperId': 'e19fd9d3262afee23c52ffb4959993b476f0934e', 'title': 'TFPnP: Tuning-free Plug-and-Play Proximal Algorithm with Applications to Inverse Imaging Problems'}, {'paperId': '268d347e8a55b5eb82fb5e7d2f800e33c75ab18a', 'title': 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'}, {'paperId': '4282b3e22aeeff913ac5fe1afdb3581c29b6234c', 'title': 'κTNG: effect of baryonic processes on weak lensing with IllustrisTNG simulations'}, {'paperId': 'd467d747dde65862d9640fe49f69d6b6d46d69ea', 'title': 'Plug-and-Play Image Restoration With Deep Denoiser Prior'}, {'paperId': '5c126ae3421f05768d8edd97ecd44b1364e2c99a', 'title': 'Denoising Diffusion Probabilistic Models'}, {'paperId': '43b1e34451f783fed053c1d539d7560dc4ec16a9', 'title': 'Implicit Neural Representations with Periodic Activation Functions'}, {'paperId': '6b5e2edad6d04702163d9b1bcbaf592294482307', 'title': 'Learning the geometry of wave-based imaging'}, {'paperId': '182665a49dc6032555823b8bea1ce2bb9237c538', 'title': 'Sparse Bayesian mass-mapping with uncertainties: Full sky observations on the celestial sphere'}, {'paperId': 'ce68c019f2b522ac6a4f7fed7c3d3bf26b00d0d9', 'title': 'Deep Local Shapes: Learning Local SDF Priors for Detailed 3D Reconstruction'}, {'paperId': '6162f4667d3ec8309767acc708be9c96e0dc7bf7', 'title': 'Local Implicit Grid Representations for 3D Scenes'}, {'paperId': '5a6732513a1dc0bea059543f208a7556e3e31067', 'title': 'Convolutional Occupancy Networks'}, {'paperId': '4c2fd562120de9cf8c60f8f8831f3de24ef4271b', 'title': 'Tuning-free Plug-and-Play Proximal Algorithm for Inverse Imaging Problems'}, {'paperId': '2b3aecd2e366a78d4da750e3634b36486daa4525', 'title': 'Learned Patch-Based Regularization for Inverse Problems in Imaging'}, {'paperId': 'a163468fd1eca5ae1aa79046670a51efcc181954', 'title': 'SAL: Sign Agnostic Learning of Shapes From Raw Data'}, {'paperId': 'd54f1c7b1e320becb0d40791a3d36dcec39adfda', 'title': 'Deep learning optoacoustic tomography with sparse data'}, {'paperId': '88bf3a55c26143e2da976560f50c0e1a24838948', 'title': 'Densely Connected Hierarchical Network for Image Denoising'}, {'paperId': '698555ce1a2fe59e2f09d5be0336ab6eed70866f', 'title': 'Invertible generative models for inverse problems: mitigating representation error and dataset bias'}, {'paperId': '88b8774fc6e072d3c29b874cbaa66ed225014970', 'title': 'Deep-Learning Schemes for Full-Wave Nonlinear Inverse Scattering Problems'}, {'paperId': '29e9fb9355f3cdc1bce868805d3fa633ab6c38ba', 'title': 'Plug-and-Play Methods for Magnetic Resonance Imaging: Using Denoisers for Image Recovery'}, {'paperId': '281789ac819eb78c9a092ebdd6d2d9097fd79515', 'title': 'First results from the TNG50 simulation: galactic outflows driven by supernovae and black hole feedback'}, {'paperId': 'c3294425af6e2c059835ec7f0dca7290b48a8faf', 'title': 'Learning Implicit Fields for Generative Shape Modeling'}, {'paperId': '21b786b3f870fc7fa247c143aa41de88b1fc6141', 'title': 'Glow: Generative Flow with Invertible 1x1 Convolutions'}, {'paperId': '2a96afaf3261a87f0daa51699b4b3cf169e092c4', 'title': 'Rotation Equivariant CNNs for Digital Pathology'}, {'paperId': '5813ae0a48ff50d48f406df52ee5b9795c34a8bd', 'title': 'Multi-level Wavelet-CNN for Image Restoration'}, {'paperId': '1c3cf4f600854fe885891a615b7ea3576ac8bc70', 'title': 'A review on CT image noise and its denoising'}, {'paperId': '8bac6c54aaaa53c8573508efddc0425e833e0560', 'title': 'Denoising Prior Driven Deep Neural Network for Image Restoration'}, {'paperId': 'd8a486a5c681ea3a6f014f3b560803adfe9a0609', 'title': 'MoDL: Model-Based Deep Learning Architecture for Inverse Problems'}, {'paperId': '744fe47157477235032f7bb3777800f9f2f45e52', 'title': 'Progressive Growing of GANs for Improved Quality, Stability, and Variation'}, {'paperId': '236e57975b6d519da24af78ea090f0bebc8c5aea', 'title': 'Convolutional Neural Networks for Inverse Problems in Imaging: A Review'}, {'paperId': '1e8e58af86b5ca7f0674155e30f4ce2b788ef776', 'title': 'Weak Lensing for Precision Cosmology'}, {'paperId': '67ebfc6a6013fcc4b64035994ad80ddfbdd6bc5e', 'title': 'Deep learning for undersampled MRI reconstruction'}, {'paperId': '96c321593aab1530455e5548a26e9629772df1f2', 'title': 'First results from the IllustrisTNG simulations: the stellar mass content of groups and clusters of galaxies'}, {'paperId': 'a64fa14fe0a7c69ce5f6cc1eceac1ecf579e1525', 'title': 'First results from the IllustrisTNG simulations: radio haloes and magnetic fields'}, {'paperId': 'cb3857dbf34cfbeebcd3614ee516e23f49b2e80f', 'title': 'First results from the IllustrisTNG simulations: matter and galaxy clustering'}, {'paperId': '1414a3c14e2ef61753100aaafdd3a00274b308a9', 'title': 'First results from the IllustrisTNG simulations: the galaxy colour bimodality'}, {'paperId': '7948ffd857e1b02d474b0474f58921674c86cd6c', 'title': 'First results from the IllustrisTNG simulations: a tale of two elements - chemical evolution of magnesium and europium'}, {'paperId': 'dc8861b4ab6799be542829ae1ace13f23cf807cd', 'title': 'Learning Deep CNN Denoiser Prior for Image Restoration'}, {'paperId': '4a73a1840945e87583d89ca0216a2c449d50a4a3', 'title': 'Deformable Convolutional Networks'}, {'paperId': 'aa618e6cd2bee89cf733383a36c73859e984df17', 'title': 'High-resolution non-destructive three-dimensional imaging of integrated circuits'}, {'paperId': 'f580b0e1020ad67bdbb11e8d99a59c21a8df1e7d', 'title': 'Compressed Sensing using Generative Models'}, {'paperId': 'db1207440d20104854464d51da4cf5f79c5de240', 'title': 'Steerable CNNs'}, {'paperId': '8a68ca7c5ba4d92de3435961f88188d0b769b920', 'title': 'Deep Convolutional Neural Network for Inverse Problems in Imaging'}, {'paperId': 'b886b8f712a122cbe6af1376981a63d51cff6de4', 'title': 'The Little Engine That Could: Regularization by Denoising (RED)'}, {'paperId': 'bd03d93de75f21a3f7574879a56ff213d92aac75', 'title': 'Robust sparse image reconstruction of radio interferometric observations with PURIFY'}, {'paperId': '0c00a328fa7cd56ee60338c54e89bd48310db80b', 'title': 'Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising'}, {'paperId': '1dc8422156c87437b2b981b742b24ab530705bb7', 'title': 'Plug-and-Play ADMM for Image Restoration: Fixed-Point Convergence and Applications'}, {'paperId': 'fafcaf5ca3fab8dc4fad15c2391c0fdb4a7dc005', 'title': 'Group Equivariant Convolutional Networks'}, {'paperId': '2c03df8b48bf3fa39054345bafabfeff15bfd11d', 'title': 'Deep Residual Learning for Image Recognition'}, {'paperId': '4dcdae25a5e33682953f0853ee4cf7ca93be58a9', 'title': 'LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop'}, {'paperId': '6364fdaa0a0eccd823a779fcdd489173f938e91a', 'title': 'U-Net: Convolutional Networks for Biomedical Image Segmentation'}, {'paperId': '558685538b914e4bb4b731956ca78c860adccf5a', 'title': 'Rotationally Invariant Image Representation for Viewing Direction Classification in Cryo-EM'}, {'paperId': '1d70937202d843664c5591fde4fb0d48627d1cf6', 'title': 'Image denoising: Can plain neural networks compete with BM3D?'}, {'paperId': '529403ab43b381b942b67751862b614cbd94341b', 'title': 'From learning models of natural image patches to whole image restoration'}, {'paperId': '07772c3d15af497a49d3c707aba7fb65be739fa9', 'title': 'Natural image denoising: Optimality and inherent bounds'}, {'paperId': '85e4dbcff0b63773db298562ae3fff258eea195f', 'title': 'Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers'}, {'paperId': 'f78244276b616c156d03d6ed913cbebec610bf08', 'title': 'An outlook on x-ray CT research and development.'}, {'paperId': 'a4c644e5cc0b1795c496211410635dda0e59cbcf', 'title': 'Computed tomography--an increasing source of radiation exposure.'}, {'paperId': '51d267b782e7caf2b6bc7240b1a5f48044ffe115', 'title': 'Image Denoising by Sparse 3-D Transform-Domain Collaborative Filtering'}, {'paperId': '9c842b2926fd60b9e6ff80fee28c65e7c1ae5f1d', 'title': 'A non-local algorithm for image denoising'}, {'paperId': '91eaeda8f3c6bf68c2c2b2e643e696b8b9460169', 'title': 'Theory of Remote Image Formation'}, {'paperId': 'eae2e0fa72e898c289365c0af16daf57a7a6cf40', 'title': 'Image quality assessment: from error visibility to structural similarity'}, {'paperId': 'f9f836d28f52ad260213d32224a6d227f8e8849a', 'title': 'Object recognition from local scale-invariant features'}, {'paperId': '16aef5def0fce8e27d07485362c050647c8444e6', 'title': 'Mapping the dark matter with weak gravitational lensing'}, {'paperId': 'cc70049623a76a7c5200494845b5d263620ee85c', 'title': 'PatchNR: Learning from Small Data by Patch Normalizing Flow Regularization'}, {'paperId': None, 'title': 'Representing scenes as neural radiance fields for view synthesis'}, {'paperId': None, 'title': 'Computed tomography images for intracranial hemorrhage detection and segmentation'}, {'paperId': None, 'title': 'Glasgow, UK, August 23–28'}, {'paperId': None, 'title': 'George W Interferometry and Synthesis in Radio Astronomy, 3rd Edition'}, {'paperId': None, 'title': 'A method for stochastic optimization'}, {'paperId': '61385083278afabcdf2e25fbc3749850ef0d2092', 'title': '5-29-2013 Plug-and-Play Priors for Model Based Reconstruction'}, {'paperId': 'a9f90916ce2d048f99d7d0d4e7dd5c1c723e6076', 'title': 'An Iterative Regularization Method for Total Variation-Based Image Restoration'}]"
3440240426,Which bits went where? Past and future transfer entropy decomposition with the information bottleneck,"Kieran A. Murphy, Zhuowen Yin, Dani S. Bassett",2024-11-07 18:57:24+00:00,"Whether the system under study is a shoal of fish, a collection of neurons,
or a set of interacting atmospheric and oceanic processes, transfer entropy
measures the flow of information between time series and can detect possible
causal relationships. Much like mutual information, transfer entropy is
generally reported as a single value summarizing an amount of shared variation,
yet a more fine-grained accounting might illuminate much about the processes
under study. Here we propose to decompose transfer entropy and localize the
bits of variation on both sides of information flow: that of the originating
process's past and that of the receiving process's future. We employ the
information bottleneck (IB) to compress the time series and identify the
transferred entropy. We apply our method to decompose the transfer entropy in
several synthetic recurrent processes and an experimental mouse dataset of
concurrent behavioral and neural activity. Our approach highlights the nuanced
dynamics within information flow, laying a foundation for future explorations
into the intricate interplay of temporal processes in complex systems.",http://arxiv.org/pdf/2411.04992v1,http://arxiv.org/abs/2411.04992v1,22,0,"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Mathematics', 'source': 'external'}, {'category': 'Physics', 'source': 's2-fos-model'}, {'category': 'Environmental Science', 'source': 's2-fos-model'}]",,[],"[{'paperId': '987aa3961eebbb1ca7a227694cb204eb491deb79', 'title': 'Rapid fluctuations in functional connectivity of cortical networks encode spontaneous behavior'}, {'paperId': '40ab612503de4c0fa0713ee47ae22930f245c9b6', 'title': 'Information decomposition in complex systems via machine learning'}, {'paperId': '718d22c746ded92caf5d79638a1b5ac6405ca200', 'title': 'Interpretability with full complexity by constraining feature information'}, {'paperId': '836966bc9bb5bdce1df9de77ca1c5f3e83373b28', 'title': 'Transfer Entropy Bottleneck: Learning Sequence to Sequence Information Transfer'}, {'paperId': '16e61facac9362e51d93f91d280d351b12c09bbf', 'title': 'What Can Local Transfer Entropy Tell Us about Phase-Amplitude Coupling in Electrophysiological Signals?'}, {'paperId': 'ac07938ba24a8dc33fcf741807c28af8bcde55a9', 'title': 'Inferring network properties from time series using transfer entropy and mutual information: Validation of multivariate versus bivariate approaches'}, {'paperId': '4233a9e49f052a2bbf2910cf9af247801710b2a8', 'title': 'The Information Bottleneck Problem and its Applications in Machine Learning'}, {'paperId': '7b983cfa013c532e8316e894ecb78d3cf3aa48b1', 'title': 'Inferring causation from time series in Earth system sciences'}, {'paperId': '4fcb5695dd968099aad2e62146efc5a7191e943c', 'title': 'CCMI : Classifier based Conditional Mutual Information Estimation'}, {'paperId': 'f6e6c948a2074e38e0a4e9099c0f63773c6013dd', 'title': 'Causality'}, {'paperId': 'b227f3e4c0dc96e5ac5426b85485a70f2175a205', 'title': 'Representation Learning with Contrastive Predictive Coding'}, {'paperId': 'a90226c41b79f8b06007609f39f82757073641e2', 'title': 'beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework'}, {'paperId': 'e44486d3f02539b818fa1451f963de82ec9b2140', 'title': 'Model-free information-theoretic approach to infer leadership in pairs of zebrafish.'}, {'paperId': '3f62b3d3e725674388067fff716e093f7807f9ef', 'title': 'Information Flows? A Critique of Transfer Entropies'}, {'paperId': 'd4eb18d616b212639926aa8c82e512aa91bd5606', 'title': 'Granger causality and transfer entropy are equivalent for Gaussian variables.'}, {'paperId': 'a0d249778bc326dd3351017fc0ba8b46c7733b4e', 'title': 'Symbolic transfer entropy.'}, {'paperId': 'c97a1d1d65e61abcce89f36b6d80381ac5e8e4f7', 'title': 'Local information transfer as a spatiotemporal filter for complex systems.'}, {'paperId': '4ef483f819e11873822416042a4b6dc4652e010c', 'title': 'The information bottleneck method'}, {'paperId': 'b86514fbdeb401509a7f1d02a06c3c592fca3f3f', 'title': 'Measuring information transfer'}, {'paperId': '6a7c63a73724c0ca68b1675e256bb8b9a35c94f4', 'title': 'Investigating causal relations by econometric models and cross-spectral methods'}, {'paperId': 'a181fb5a42ad8fe2cc27b5542fa40384e9a8d72c', 'title': 'Deep Variational Information Bottleneck'}, {'paperId': '557668619327081a6b77aa5b181fa84722a875a4', 'title': 'CAUSALITY, FEEDBACK AND DIRECTED INFORMATION'}]"
3465098052,Clustering in Causal Attention Masking,"Nikita Karagodin, Yury Polyanskiy, Philippe Rigollet",2024-11-07 18:56:37+00:00,"This work presents a modification of the self-attention dynamics proposed by
Geshkovski et al. (arXiv:2312.10794) to better reflect the practically
relevant, causally masked attention used in transformer architectures for
generative AI. This modification translates into an interacting particle system
that cannot be interpreted as a mean-field gradient flow. Despite this loss of
structure, we significantly strengthen the results of Geshkovski et al.
(arXiv:2312.10794) in this context: While previous rigorous results focused on
cases where all three matrices (Key, Query, and Value) were scaled identities,
we prove asymptotic convergence to a single cluster for arbitrary key-query
matrices and a value matrix equal to the identity. Additionally, we establish a
connection to the classical R\'enyi parking problem from combinatorial geometry
to make initial theoretical steps towards demonstrating the existence of
meta-stable states.",http://arxiv.org/pdf/2411.04990v1,http://arxiv.org/abs/2411.04990v1,35,0,"[{'category': 'Computer Science', 'source': 'external'}, {'category': 'Mathematics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}]","{'model': 'tldr@v2.0.0', 'text': 'This work presents a modification of the self-attention dynamics proposed by Geshkovski et al. to better reflect the practically relevant, causally masked attention used in transformer architectures for generative AI, and proves asymptotic convergence to a single cluster for arbitrary key-query matrices and a value matrix equal to the identity.'}",[],"[{'paperId': '04733d602517cc4809f0053d8fb91f9b031b6eb7', 'title': 'Emergence of meta-stable clustering in mean-field transformer models'}, {'paperId': '39219fdb253e1dc59f81c15b2b10bedd8cd15893', 'title': 'Dynamic metastability in the self-attention model'}, {'paperId': '893fccf0f24bd52be4a309b12e77429a9712a446', 'title': 'Synchronization on circles and spheres with nonlinear interactions'}, {'paperId': '32112b798f70faab00e14806f51d46058cf5e597', 'title': 'Chameleon: Mixed-Modal Early-Fusion Foundation Models'}, {'paperId': '7572ba7f604ef95d7acdd657ebac458106bd35df', 'title': 'Accurate structure prediction of biomolecular interactions with AlphaFold 3'}, {'paperId': '27cc3c9d1854c9bebc495276317483bfc173510a', 'title': 'Generic controllability of equivariant systems and applications to particle systems and neural networks'}, {'paperId': '3e4785c6153d7d96304911254443187f5c66d913', 'title': 'Geometric Dynamics of Signal Propagation Predict Trainability of Transformers'}, {'paperId': '2cb91c1a9488c3bdb05eee7668ac4a77c7e18a3b', 'title': 'How Smooth Is Attention?'}, {'paperId': '54c7bdf63719bb366987bb6e9e857335a479f4d9', 'title': 'A mathematical perspective on Transformers'}, {'paperId': '8b9f01585a679dffe92261ecdec56425db9ef97f', 'title': 'Demystifying Oversmoothing in Attention-Based Graph Neural Networks'}, {'paperId': 'e5526fcee23b6dc7b7bf0d83607d88d12cf6baf2', 'title': 'The emergence of clusters in self-attention dynamics'}, {'paperId': '2a3213cb3c755f036d5dfec7261d726a819c78c1', 'title': 'Muse: Text-To-Image Generation via Masked Generative Transformers'}, {'paperId': 'a26d86e702763d63245c22cbe4a019fa5f0f4c49', 'title': 'Expander graphs are globally synchronizing'}, {'paperId': '9f96564f417d778b38a4f440345902f689a00793', 'title': 'From microscopic to macroscopic scale equations: mean field, hydrodynamic and graph limits'}, {'paperId': '1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe', 'title': 'Scaling Autoregressive Models for Content-Rich Text-to-Image Generation'}, {'paperId': 'fe9d978f7718474e9613bac114c398614f09be71', 'title': 'Sinkformers: Transformers with Doubly Stochastic Attention'}, {'paperId': 'dfb37e6216e792bf6bd5a30c0fc7ad55df1cb71e', 'title': 'Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth'}, {'paperId': '2cd605106b88c85d7d8b865b1ef0f8c8293debf1', 'title': 'Zero-Shot Text-to-Image Generation'}, {'paperId': 'd58a80bb515b3ddc2cfd3503e1d493d093ed0640', 'title': 'On the Regularity of Attention'}, {'paperId': '47f7ec3d0a5e6e83b6768ece35206a94dc81919c', 'title': 'Taming Transformers for High-Resolution Image Synthesis'}, {'paperId': '2ac1aa60554f26afafc6135d095c7a7f397a5f78', 'title': 'Clustering'}, {'paperId': '10eda4521c032adabaa8e70d6569e17370b29dcd', 'title': 'Root Mean Square Layer Normalization'}, {'paperId': '7a064df1aeada7e69e5173f7d4c8606f4470365b', 'title': 'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations'}, {'paperId': '204e3073870fae3d05bcbc2f6a8e263d9b72e776', 'title': 'Attention is All you Need'}, {'paperId': '54463d9ff24737484888312e8e951523b9365cb8', 'title': 'The problem of almost global consensus on the n-sphere'}, {'paperId': 'ba800f46230f8522be574e23f3c1e00f398ccaae', 'title': 'Some applications of the Łojasiewicz gradient inequality'}, {'paperId': '52e9f7e03e5d8234bcf61ade5aea0e1db5f12536', 'title': 'From Kuramoto to Crawford: exploring the onset of synchronization in populations of coupled oscillators'}, {'paperId': '8f0cadf3ddaa4b92b6982cef8fdc35d359609e78', 'title': 'Global Stability of Dynamical Systems'}, {'paperId': 'f2dfaf6e3f3e307abeaab91d13b4e1c2deb1f77a', 'title': 'A Parking Problem'}, {'paperId': None, 'title': 'Residual connections harm self-supervised abstract feature learning'}, {'paperId': None, 'title': 'Sur les trajectoires du gradient d’une fonction analytique'}, {'paperId': '3e686d19248350f6dd23298dc2e2c94090584dd0', 'title': 'Ensembles semi-analytiques'}, {'paperId': None, 'title': 'Une propriété topologique des sous-ensembles analytiques réels'}, {'paperId': None, 'title': 'On a one-dimensional problem concerning space-filling'}, {'paperId': None, 'title': 'eigenvalue of V with the largest real part as λ max'}]"
